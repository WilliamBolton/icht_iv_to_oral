{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1677891129718
        }
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 40)\n",
        "pd.set_option('display.width', 2000)\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from sktime.transformations.panel.catch22 import Catch22\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1677864854376
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/antibiotic_vitals.csv'\n",
        "antibiotic_vitals_pivoted_4 = pd.read_csv(path)\n",
        "antibiotic_vitals_pivoted_4['hours_grouped'] = antibiotic_vitals_pivoted_4['hours_grouped'].astype(int) # Needs to be type int for c22 to work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1677864854726
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Remove those without enough data\n",
        "c22_other = antibiotic_vitals_pivoted_4[antibiotic_vitals_pivoted_4['SPELL_IDENTIFIER'].isin((antibiotic_vitals_pivoted_4.groupby('SPELL_IDENTIFIER').size() > 8).where(lambda x : x == True).dropna().reset_index()['SPELL_IDENTIFIER'].to_list())]\n",
        "# Set index\n",
        "c22_other.set_index(['SPELL_IDENTIFIER', 'date', 'hours_grouped'] ,inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1677864854967
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1677864855198
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def c22_other_fun(df):\n",
        "    c22 = Catch22(catch24=True)\n",
        "    master_df = pd.DataFrame()\n",
        "    overlord_df = pd.DataFrame()\n",
        "    for x in range(len(df.columns)): # Iterate through columns sas this c22 does not work on multiple columns\n",
        "        print('column number:', x)\n",
        "        working_df = df.iloc[:, x]\n",
        "        working_df = working_df.to_frame()\n",
        "        for column_name in working_df:\n",
        "            master_df = pd.DataFrame()\n",
        "            master_df2 = pd.DataFrame()\n",
        "            for stay_id, new_df in working_df[[column_name]].groupby(level=0, dropna=False): # Iterate through stays\n",
        "\n",
        "                n_list = list(range(1, math.ceil(len(new_df)/4)+1))\n",
        "                for n in range(1, math.ceil(len(new_df)/4)+1): # Do this to understand how many 'days' / sets of data we will have for each stay beyond 24 and 48 hours\n",
        "                    # Ignore first 48 hours as calculated seperatly\n",
        "                    if n == 1:\n",
        "                        continue\n",
        "                    elif n == 2:\n",
        "                        continue\n",
        "                    # This is for exceptional cases where we want hours grouped 1 for last day and the spell starts on hours grouped 2\n",
        "                    elif (new_df.reset_index().iloc[0]['hours_grouped'] == 2) and ((n == (len(new_df)/4)) and (n == n_list[-1])):\n",
        "                        exit_flag = True\n",
        "                        flag_2 = False\n",
        "                        flag_1 = False\n",
        "                        # Get index of nth 2 hours grouped value\n",
        "                        try:\n",
        "                            index_value = new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index][new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index]['hours_grouped'] == 2].index.values[0]\n",
        "                            new_df2 = new_df.iloc[:index_value+1]\n",
        "                            exit_flag = False\n",
        "                            flag_2 = True\n",
        "                        #  Look at nth 1 hours grouped  \n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                        try:\n",
        "                            index_value = new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index][new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index]['hours_grouped'] == 1].index.values[0]\n",
        "                            new_df3 = new_df.iloc[:index_value+1]\n",
        "                            exit_flag = False\n",
        "                            flag_1 = True\n",
        "                        except:\n",
        "                            pass\n",
        "                        \n",
        "                        if exit_flag == True:\n",
        "                            continue\n",
        "                        \n",
        "                        if flag_2 == True:\n",
        "                            new_df2_2 = new_df2.tail(4).reset_index(drop=True).dropna()\n",
        "                            new_df2_3 = new_df2.reset_index(drop=True).dropna()\n",
        "                    \n",
        "                            # C22 for latest day\n",
        "                            if len(new_df2_2) <= 2: # c22 only works with 3 or more timepoints\n",
        "                                transformed_data = pd.DataFrame()\n",
        "                                np_data = new_df2_2.to_numpy()\n",
        "                                if not np_data.size == 0:\n",
        "                                    transformed_data['22'] = np.mean(np_data)\n",
        "                                    transformed_data['23'] = np.std(np_data) \n",
        "                            else: \n",
        "                                transformed_data = c22.fit_transform(new_df2_2)\n",
        "\n",
        "                            transformed_data = transformed_data.add_prefix(column_name)           \n",
        "                            transformed_data.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                            transformed_data.insert(1, 'date', new_df2.reset_index().iloc[-1]['date'])\n",
        "\n",
        "                            # C22 for all data to date for stay\n",
        "                            if len(new_df2_3) <= 2: # c22 only works with 3 or more timepoints\n",
        "                                transformed_data2 = pd.DataFrame()\n",
        "                                np_data = new_df2_3.to_numpy()\n",
        "                                if not np_data.size == 0:\n",
        "                                    transformed_data2['22'] = np.mean(np_data)\n",
        "                                    transformed_data2['23'] = np.std(np_data)\n",
        "                            else: \n",
        "                                transformed_data2 = c22.fit_transform(new_df2_3)\n",
        "\n",
        "                            transformed_data2 = transformed_data2.add_prefix(column_name)  \n",
        "                            transformed_data2 = transformed_data2.add_suffix('_current_stay') # Indicate different as temporal over whole of current stay             \n",
        "                            transformed_data2.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                            transformed_data2.insert(1, 'date', new_df2.reset_index().iloc[-1]['date'])\n",
        "\n",
        "                        else:\n",
        "                            transformed_data = pd.DataFrame()\n",
        "                            transformed_data2 = pd.DataFrame()\n",
        "                        \n",
        "                        if flag_1 == True:\n",
        "                            new_df3_2 = new_df3.tail(4).reset_index(drop=True).dropna()\n",
        "                            new_df3_3 = new_df3.reset_index(drop=True).dropna()\n",
        "\n",
        "                            # C22 for latest day\n",
        "                            if len(new_df3_2) <= 2: # c22 only works with 3 or more timepoints\n",
        "                                transformed_data3 = pd.DataFrame()\n",
        "                                np_data = new_df3_2.to_numpy()\n",
        "                                if not np_data.size == 0:\n",
        "                                    transformed_data3['22'] = np.mean(np_data)\n",
        "                                    transformed_data3['23'] = np.std(np_data)\n",
        "                            else: \n",
        "                                transformed_data3 = c22.fit_transform(new_df3_2)\n",
        "\n",
        "                            transformed_data3 = transformed_data3.add_prefix(column_name)           \n",
        "                            transformed_data3.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                            transformed_data3.insert(1, 'date', new_df.reset_index().iloc[-1]['date'])\n",
        "\n",
        "                            # C22 for all data to date for stay\n",
        "                            if len(new_df3_3) <= 2: # c22 only works with 3 or more timepoints\n",
        "                                transformed_data4 = pd.DataFrame()\n",
        "                                np_data = new_df3_3.to_numpy()\n",
        "                                if not np_data.size == 0:\n",
        "                                    transformed_data4['22'] = np.mean(np_data)\n",
        "                                    transformed_data4['23'] = np.std(np_data) \n",
        "                            else: \n",
        "                                transformed_data4 = c22.fit_transform(new_df3_3)\n",
        "                                transformed_data4 = transformed_data4.add_prefix(column_name)  \n",
        "                                transformed_data4 = transformed_data4.add_suffix('_current_stay') # Indicate different as temporal over whole of current stay             \n",
        "                                transformed_data4.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                                transformed_data4.insert(1, 'date', new_df3.reset_index().iloc[-1]['date'])\n",
        "                        else:\n",
        "                            transformed_data3 = pd.DataFrame()\n",
        "                            transformed_data4 = pd.DataFrame()\n",
        "                        \n",
        "                        # Combine\n",
        "                        transformed_data = pd.concat([transformed_data, transformed_data3], ignore_index=True)\n",
        "                        transformed_data2 = pd.concat([transformed_data2, transformed_data4], ignore_index=True)\n",
        "\n",
        "                        # Create master df's\n",
        "                        master_df = pd.concat([master_df, transformed_data])\n",
        "                        master_df2 = pd.concat([master_df2, transformed_data2])\n",
        "\n",
        "\n",
        "                    # This is for most cases  \n",
        "                    else:\n",
        "                        # Get index of nth 2 hours grouped value\n",
        "                        try:\n",
        "                            index_value = new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index][new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index]['hours_grouped'] == 2].index.values[0]\n",
        "                            new_df2 = new_df.iloc[:index_value+1]\n",
        "                        except:\n",
        "                            # If fails look at nth 1 hours grouped  \n",
        "                            try:\n",
        "                                index_value = new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index][new_df.reset_index().iloc[(new_df.reset_index().groupby('hours_grouped').cumcount() == n-1)[new_df.reset_index().groupby('hours_grouped').cumcount() == n-1].index]['hours_grouped'] == 1].index.values[0]\n",
        "                                new_df2 = new_df.iloc[:index_value+1]\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "                        new_df2_2 = new_df2.tail(4).reset_index(drop=True).dropna()\n",
        "                        new_df2_3 = new_df2.reset_index(drop=True).dropna()\n",
        "                    \n",
        "                        # C22 for latest day\n",
        "                        if len(new_df2_2) <= 2: # c22 only works with 3 or more timepoints\n",
        "                            transformed_data = pd.DataFrame()\n",
        "                            np_data = new_df2_2.to_numpy()\n",
        "                            if not np_data.size == 0:\n",
        "                                transformed_data['22'] = np.mean(np_data)\n",
        "                                transformed_data['23'] = np.std(np_data) \n",
        "                        else: \n",
        "                            transformed_data = c22.fit_transform(new_df2_2)                        \n",
        "                        \n",
        "                        transformed_data = transformed_data.add_prefix(column_name)           \n",
        "                        transformed_data.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                        transformed_data.insert(1, 'date', new_df2.reset_index().iloc[-1]['date'])\n",
        "\n",
        "                        # C22 for all data to date for stay\n",
        "                        if len(new_df2_3) <= 2: # c22 only works with 3 or more timepoints\n",
        "                            transformed_data2 = pd.DataFrame()\n",
        "                            np_data = new_df2_3.to_numpy()\n",
        "                            if not np_data.size == 0:\n",
        "                                transformed_data2['22'] = np.mean(np_data)\n",
        "                                transformed_data2['23'] = np.std(np_data)\n",
        "                        else: \n",
        "                            transformed_data2 = c22.fit_transform(new_df2_3)                        \n",
        "\n",
        "                        transformed_data2 = transformed_data2.add_prefix(column_name)  \n",
        "                        transformed_data2 = transformed_data2.add_suffix('_current_stay') # Indicate different as temporal over whole of current stay             \n",
        "                        transformed_data2.insert(0, 'SPELL_IDENTIFIER', stay_id)\n",
        "                        transformed_data2.insert(1, 'date', new_df2.reset_index().iloc[-1]['date'])\n",
        "\n",
        "                        # Create master df's\n",
        "                        master_df = pd.concat([master_df, transformed_data])\n",
        "                        master_df2 = pd.concat([master_df2, transformed_data2])\n",
        "\n",
        "            master_df.reset_index(inplace=True, drop=True)\n",
        "            master_df2.reset_index(inplace=True, drop=True)\n",
        "\n",
        "            if master_df.empty:\n",
        "                if master_df2.empty:\n",
        "                    continue\n",
        "                else:\n",
        "                    master_df = master_df2\n",
        "            else:\n",
        "                master_df = master_df.merge(master_df2, how='left', on=['SPELL_IDENTIFIER', 'date'])\n",
        "\n",
        "        if x == 0:\n",
        "            overlord_df = master_df.copy()\n",
        "        elif master_df.empty:\n",
        "            continue\n",
        "        else:\n",
        "            overlord_df = overlord_df.merge(master_df, how='outer', on=['SPELL_IDENTIFIER', 'date'])\n",
        "            overlord_df.dropna(axis=0, how='all', inplace=True)\n",
        "        \n",
        "        # Save as go through\n",
        "        overlord_df.to_csv('switch_data/new_c22_other_days.csv', index=False)\n",
        "\n",
        "    return overlord_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1677878425470
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "column number: 0\n",
            "column number: 1\n",
            "column number: 2\n",
            "column number: 3\n",
            "column number: 4\n",
            "column number: 5\n",
            "column number: 6\n",
            "column number: 7\n",
            "column number: 8\n",
            "column number: 9\n"
          ]
        }
      ],
      "source": [
        "c22_other_days_df = c22_other_fun(c22_other)\n",
        "# Save\n",
        "c22_other_days_df.to_csv('switch_data/c22_other_days_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1677880684010
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "c22_other_days_df.drop(columns=['22_x', '23_x', '22_y', '23_y'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1677880717561
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "#c22_other_days_df.to_csv('switch_data/c22_other_days_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Combine and work out difference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1677884211279
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/c22_24_hour.csv'\n",
        "c22_24_hour = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1677884213397
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/c22_48_hour.csv'\n",
        "c22_48_hour = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1677884334538
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "c22_combined = pd.concat([c22_24_hour, c22_48_hour, c22_other_days_df])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1677884614416
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Move column\n",
        "col = c22_combined.pop(\"48_hour_flag\")\n",
        "c22_combined.insert(3, col.name, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "gather": {
          "logged": 1677889997999
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "c22_combined2 = c22_combined.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1677884756925
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Order\n",
        "c22_combined = c22_combined.sort_values(by=['SPELL_IDENTIFIER', 'date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "gather": {
          "logged": 1677890722779
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Difference\n",
        "c22_combined3 = c22_combined2.drop(columns=['date', '24_hour_flag', '48_hour_flag'])\n",
        "\n",
        "c22_combined_diff = pd.DataFrame()\n",
        "c22_combined3.set_index('SPELL_IDENTIFIER', inplace=True)\n",
        "for stay_id, new_df in c22_combined3.groupby(level=0):\n",
        "    diff_df = new_df.diff()\n",
        "    diff_df = diff_df.add_suffix('_difference')\n",
        "    c22_combined_diff = pd.concat([c22_combined_diff, diff_df], ignore_index=True)\n",
        "\n",
        "c22_combined3.reset_index(inplace=True)\n",
        "c22_combined_diff = pd.concat([c22_combined3, c22_combined_diff], axis=1)\n",
        "c22_combined_diff = pd.concat([c22_combined2[['date', '24_hour_flag', '48_hour_flag']], c22_combined_diff], axis=1)\n",
        "col = c22_combined_diff.pop(\"SPELL_IDENTIFIER\")\n",
        "c22_combined_diff.insert(0, col.name, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "gather": {
          "logged": 1677890824792
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "#c22_combined_diff.to_csv('switch_data/c22_all_with_diff.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1677891162805
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/c22_all_with_diff.csv'\n",
        "c22_all_with_diff = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1677891215277
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10007"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "c22_all_with_diff['SPELL_IDENTIFIER'].nunique()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
