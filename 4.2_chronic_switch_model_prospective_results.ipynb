{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1711640763261
        }
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 40)\n",
        "pd.set_option('display.width', 2000)\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Remove printing error\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1711640772445
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fda3bedf930>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ],
      "source": [
        "# Set the random seeds for deterministic results.\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1711640773743
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/chronic_switch_icare_df_preprocessed_2023.csv'\n",
        "icare_df_preprocessed = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1711640774077
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "547"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "icare_df_preprocessed.SPELL_IDENTIFIER.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Main run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1711640774438
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def set_transformer_processing_fun(patient_df, snomed_embedding):\n",
        "    # Str\n",
        "    patient_df.columns = patient_df.columns.astype(str)\n",
        "    snomed_embedding['snomed_code'] = snomed_embedding['snomed_code'].astype(str)\n",
        "    # Filter\n",
        "    snomed_embedding = snomed_embedding[snomed_embedding['snomed_code'].isin(patient_df.columns.tolist())]\n",
        "    snomed_embedding.set_index('snomed_code', inplace=True)\n",
        "    # Get lengths of each patients co-morbidities\n",
        "    comorbidity_len = np.array(patient_df.sum(axis=1))\n",
        "    # Add padding embedding \n",
        "    padding_df = pd.DataFrame(np.random.choice([0], size=len(snomed_embedding.columns))) # Changed to 0\n",
        "    padding_df = padding_df.T\n",
        "    padding_df.index = ['9999999999']\n",
        "    padding_df.columns = snomed_embedding.columns\n",
        "    snomed_embedding2 = pd.concat([snomed_embedding, padding_df])\n",
        "    snomed_embedding2.index = snomed_embedding2.index.astype(str)\n",
        "    # Get max number of co-morbidities\n",
        "    max_len = 22 # Define for same for all splits (train val etr)\n",
        "    # Format patients embeddings into set and pad / create array\n",
        "    feature_array = np.zeros(shape=(len(patient_df), max_len , 128))\n",
        "    n = -1\n",
        "    for index, row in patient_df.iterrows():\n",
        "        n += 1\n",
        "        n2 = -1\n",
        "        code_list = row[row ==1].index.tolist()\n",
        "        while len(code_list) < max_len:\n",
        "            code_list.append('9999999999')\n",
        "        for code in code_list:\n",
        "            n2 += 1\n",
        "            feature_array[n, n2] = np.array(snomed_embedding2.loc[code])\n",
        "    \n",
        "    # Create mask tensor based on lengths\n",
        "    comorbidity_len2 = torch.as_tensor(comorbidity_len, dtype=torch.long)\n",
        "    mask = torch.arange(max_len)[None, :] < comorbidity_len2[:, None]\n",
        "\n",
        "    return feature_array, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1711640774763
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define how long an epoch takes\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# Initializing the weights of our model.\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "# Calculate the number of trainable parameters in the model.\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1711640775487
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\n",
        " 'Diastolic Blood Pressure2',\n",
        " 'Diastolic Blood Pressure3',\n",
        " 'Diastolic Blood Pressure4',\n",
        " 'Diastolic Blood Pressure5',\n",
        " 'Diastolic Blood Pressure6',\n",
        " 'Diastolic Blood Pressure7',\n",
        " 'Diastolic Blood Pressure8',\n",
        " 'Diastolic Blood Pressure9',\n",
        " 'Diastolic Blood Pressure11',\n",
        " 'Diastolic Blood Pressure12',\n",
        " 'Diastolic Blood Pressure13',\n",
        " 'Diastolic Blood Pressure14',\n",
        " 'Diastolic Blood Pressure15',\n",
        " 'Diastolic Blood Pressure16',\n",
        " 'Diastolic Blood Pressure18',\n",
        " 'Diastolic Blood Pressure19',\n",
        " 'Diastolic Blood Pressure20',\n",
        " 'Diastolic Blood Pressure21',\n",
        " 'Diastolic Blood Pressure2_current_stay',\n",
        " 'Diastolic Blood Pressure3_current_stay',\n",
        " 'Diastolic Blood Pressure4_current_stay',\n",
        " 'Diastolic Blood Pressure5_current_stay',\n",
        " 'Diastolic Blood Pressure6_current_stay',\n",
        " 'Diastolic Blood Pressure8_current_stay',\n",
        " 'Diastolic Blood Pressure12_current_stay',\n",
        " 'Diastolic Blood Pressure13_current_stay',\n",
        " 'Diastolic Blood Pressure14_current_stay',\n",
        " 'Diastolic Blood Pressure16_current_stay',\n",
        " 'Diastolic Blood Pressure18_current_stay',\n",
        " 'Diastolic Blood Pressure19_current_stay',\n",
        " 'Diastolic Blood Pressure20_current_stay',\n",
        " 'Diastolic Blood Pressure21_current_stay',\n",
        " 'Glasgow Coma Score0',\n",
        " 'Glasgow Coma Score1',\n",
        " 'Glasgow Coma Score2',\n",
        " 'Glasgow Coma Score3',\n",
        " 'Glasgow Coma Score4',\n",
        " 'Glasgow Coma Score5',\n",
        " 'Glasgow Coma Score6',\n",
        " 'Glasgow Coma Score7',\n",
        " 'Glasgow Coma Score8',\n",
        " 'Glasgow Coma Score9',\n",
        " 'Glasgow Coma Score10',\n",
        " 'Glasgow Coma Score11',\n",
        " 'Glasgow Coma Score12',\n",
        " 'Glasgow Coma Score13',\n",
        " 'Glasgow Coma Score14',\n",
        " 'Glasgow Coma Score15',\n",
        " 'Glasgow Coma Score16',\n",
        " 'Glasgow Coma Score17',\n",
        " 'Glasgow Coma Score18',\n",
        " 'Glasgow Coma Score19',\n",
        " 'Glasgow Coma Score20',\n",
        " 'Glasgow Coma Score21',\n",
        " 'Glasgow Coma Score0_current_stay',\n",
        " 'Glasgow Coma Score1_current_stay',\n",
        " 'Glasgow Coma Score2_current_stay',\n",
        " 'Glasgow Coma Score3_current_stay',\n",
        " 'Glasgow Coma Score4_current_stay',\n",
        " 'Glasgow Coma Score5_current_stay',\n",
        " 'Glasgow Coma Score6_current_stay',\n",
        " 'Glasgow Coma Score7_current_stay',\n",
        " 'Glasgow Coma Score8_current_stay',\n",
        " 'Glasgow Coma Score10_current_stay',\n",
        " 'Glasgow Coma Score11_current_stay',\n",
        " 'Glasgow Coma Score12_current_stay',\n",
        " 'Glasgow Coma Score13_current_stay',\n",
        " 'Glasgow Coma Score14_current_stay',\n",
        " 'Glasgow Coma Score15_current_stay',\n",
        " 'Glasgow Coma Score16_current_stay',\n",
        " 'Glasgow Coma Score17_current_stay',\n",
        " 'Glasgow Coma Score18_current_stay',\n",
        " 'Glasgow Coma Score19_current_stay',\n",
        " 'Glasgow Coma Score20_current_stay',\n",
        " 'Glasgow Coma Score21_current_stay',\n",
        " 'Heart Rate2',\n",
        " 'Heart Rate3',\n",
        " 'Heart Rate4',\n",
        " 'Heart Rate5',\n",
        " 'Heart Rate6',\n",
        " 'Heart Rate7',\n",
        " 'Heart Rate8',\n",
        " 'Heart Rate9',\n",
        " 'Heart Rate11',\n",
        " 'Heart Rate12',\n",
        " 'Heart Rate13',\n",
        " 'Heart Rate14',\n",
        " 'Heart Rate15',\n",
        " 'Heart Rate16',\n",
        " 'Heart Rate18',\n",
        " 'Heart Rate19',\n",
        " 'Heart Rate20',\n",
        " 'Heart Rate21',\n",
        " 'Heart Rate2_current_stay',\n",
        " 'Heart Rate3_current_stay',\n",
        " 'Heart Rate4_current_stay',\n",
        " 'Heart Rate5_current_stay',\n",
        " 'Heart Rate6_current_stay',\n",
        " 'Heart Rate8_current_stay',\n",
        " 'Heart Rate12_current_stay',\n",
        " 'Heart Rate13_current_stay',\n",
        " 'Heart Rate14_current_stay',\n",
        " 'Heart Rate16_current_stay',\n",
        " 'Heart Rate18_current_stay',\n",
        " 'Heart Rate19_current_stay',\n",
        " 'Heart Rate20_current_stay',\n",
        " 'Heart Rate21_current_stay',\n",
        " 'Mean Arterial Pressure2',\n",
        " 'Mean Arterial Pressure3',\n",
        " 'Mean Arterial Pressure4',\n",
        " 'Mean Arterial Pressure5',\n",
        " 'Mean Arterial Pressure6',\n",
        " 'Mean Arterial Pressure7',\n",
        " 'Mean Arterial Pressure8',\n",
        " 'Mean Arterial Pressure9',\n",
        " 'Mean Arterial Pressure11',\n",
        " 'Mean Arterial Pressure12',\n",
        " 'Mean Arterial Pressure13',\n",
        " 'Mean Arterial Pressure14',\n",
        " 'Mean Arterial Pressure15',\n",
        " 'Mean Arterial Pressure16',\n",
        " 'Mean Arterial Pressure18',\n",
        " 'Mean Arterial Pressure19',\n",
        " 'Mean Arterial Pressure20',\n",
        " 'Mean Arterial Pressure21',\n",
        " 'Mean Arterial Pressure2_current_stay',\n",
        " 'Mean Arterial Pressure3_current_stay',\n",
        " 'Mean Arterial Pressure4_current_stay',\n",
        " 'Mean Arterial Pressure5_current_stay',\n",
        " 'Mean Arterial Pressure6_current_stay',\n",
        " 'Mean Arterial Pressure8_current_stay',\n",
        " 'Mean Arterial Pressure12_current_stay',\n",
        " 'Mean Arterial Pressure13_current_stay',\n",
        " 'Mean Arterial Pressure14_current_stay',\n",
        " 'Mean Arterial Pressure16_current_stay',\n",
        " 'Mean Arterial Pressure18_current_stay',\n",
        " 'Mean Arterial Pressure19_current_stay',\n",
        " 'Mean Arterial Pressure20_current_stay',\n",
        " 'Mean Arterial Pressure21_current_stay',\n",
        " 'NEWS Conscious Level Score0',\n",
        " 'NEWS Conscious Level Score1',\n",
        " 'NEWS Conscious Level Score2',\n",
        " 'NEWS Conscious Level Score3',\n",
        " 'NEWS Conscious Level Score4',\n",
        " 'NEWS Conscious Level Score5',\n",
        " 'NEWS Conscious Level Score6',\n",
        " 'NEWS Conscious Level Score7',\n",
        " 'NEWS Conscious Level Score8',\n",
        " 'NEWS Conscious Level Score9',\n",
        " 'NEWS Conscious Level Score10',\n",
        " 'NEWS Conscious Level Score11',\n",
        " 'NEWS Conscious Level Score12',\n",
        " 'NEWS Conscious Level Score13',\n",
        " 'NEWS Conscious Level Score14',\n",
        " 'NEWS Conscious Level Score15',\n",
        " 'NEWS Conscious Level Score16',\n",
        " 'NEWS Conscious Level Score17',\n",
        " 'NEWS Conscious Level Score18',\n",
        " 'NEWS Conscious Level Score19',\n",
        " 'NEWS Conscious Level Score20',\n",
        " 'NEWS Conscious Level Score21',\n",
        " 'NEWS Conscious Level Score0_current_stay',\n",
        " 'NEWS Conscious Level Score1_current_stay',\n",
        " 'NEWS Conscious Level Score2_current_stay',\n",
        " 'NEWS Conscious Level Score3_current_stay',\n",
        " 'NEWS Conscious Level Score4_current_stay',\n",
        " 'NEWS Conscious Level Score5_current_stay',\n",
        " 'NEWS Conscious Level Score6_current_stay',\n",
        " 'NEWS Conscious Level Score7_current_stay',\n",
        " 'NEWS Conscious Level Score8_current_stay',\n",
        " 'NEWS Conscious Level Score9_current_stay',\n",
        " 'NEWS Conscious Level Score10_current_stay',\n",
        " 'NEWS Conscious Level Score11_current_stay',\n",
        " 'NEWS Conscious Level Score12_current_stay',\n",
        " 'NEWS Conscious Level Score13_current_stay',\n",
        " 'NEWS Conscious Level Score14_current_stay',\n",
        " 'NEWS Conscious Level Score15_current_stay',\n",
        " 'NEWS Conscious Level Score16_current_stay',\n",
        " 'NEWS Conscious Level Score17_current_stay',\n",
        " 'NEWS Conscious Level Score18_current_stay',\n",
        " 'NEWS Conscious Level Score19_current_stay',\n",
        " 'NEWS Conscious Level Score20_current_stay',\n",
        " 'NEWS Conscious Level Score21_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc0',\n",
        " 'NEWS Supplemental Oxygen Calc1',\n",
        " 'NEWS Supplemental Oxygen Calc2',\n",
        " 'NEWS Supplemental Oxygen Calc3',\n",
        " 'NEWS Supplemental Oxygen Calc4',\n",
        " 'NEWS Supplemental Oxygen Calc5',\n",
        " 'NEWS Supplemental Oxygen Calc6',\n",
        " 'NEWS Supplemental Oxygen Calc7',\n",
        " 'NEWS Supplemental Oxygen Calc8',\n",
        " 'NEWS Supplemental Oxygen Calc9',\n",
        " 'NEWS Supplemental Oxygen Calc10',\n",
        " 'NEWS Supplemental Oxygen Calc11',\n",
        " 'NEWS Supplemental Oxygen Calc12',\n",
        " 'NEWS Supplemental Oxygen Calc13',\n",
        " 'NEWS Supplemental Oxygen Calc14',\n",
        " 'NEWS Supplemental Oxygen Calc15',\n",
        " 'NEWS Supplemental Oxygen Calc16',\n",
        " 'NEWS Supplemental Oxygen Calc17',\n",
        " 'NEWS Supplemental Oxygen Calc18',\n",
        " 'NEWS Supplemental Oxygen Calc19',\n",
        " 'NEWS Supplemental Oxygen Calc20',\n",
        " 'NEWS Supplemental Oxygen Calc21',\n",
        " 'NEWS Supplemental Oxygen Calc0_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc1_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc2_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc3_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc4_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc5_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc6_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc7_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc8_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc9_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc10_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc11_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc12_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc13_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc14_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc15_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc16_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc17_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc18_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc19_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc20_current_stay',\n",
        " 'NEWS Supplemental Oxygen Calc21_current_stay',\n",
        " 'Respiratory Rate0',\n",
        " 'Respiratory Rate2',\n",
        " 'Respiratory Rate3',\n",
        " 'Respiratory Rate4',\n",
        " 'Respiratory Rate5',\n",
        " 'Respiratory Rate6',\n",
        " 'Respiratory Rate7',\n",
        " 'Respiratory Rate8',\n",
        " 'Respiratory Rate9',\n",
        " 'Respiratory Rate11',\n",
        " 'Respiratory Rate12',\n",
        " 'Respiratory Rate13',\n",
        " 'Respiratory Rate14',\n",
        " 'Respiratory Rate15',\n",
        " 'Respiratory Rate16',\n",
        " 'Respiratory Rate18',\n",
        " 'Respiratory Rate19',\n",
        " 'Respiratory Rate20',\n",
        " 'Respiratory Rate21',\n",
        " 'Respiratory Rate0_current_stay',\n",
        " 'Respiratory Rate2_current_stay',\n",
        " 'Respiratory Rate3_current_stay',\n",
        " 'Respiratory Rate4_current_stay',\n",
        " 'Respiratory Rate5_current_stay',\n",
        " 'Respiratory Rate6_current_stay',\n",
        " 'Respiratory Rate8_current_stay',\n",
        " 'Respiratory Rate12_current_stay',\n",
        " 'Respiratory Rate13_current_stay',\n",
        " 'Respiratory Rate14_current_stay',\n",
        " 'Respiratory Rate16_current_stay',\n",
        " 'Respiratory Rate18_current_stay',\n",
        " 'Respiratory Rate19_current_stay',\n",
        " 'Respiratory Rate20_current_stay',\n",
        " 'Respiratory Rate21_current_stay',\n",
        " 'SpO20',\n",
        " 'SpO22',\n",
        " 'SpO23',\n",
        " 'SpO24',\n",
        " 'SpO25',\n",
        " 'SpO26',\n",
        " 'SpO27',\n",
        " 'SpO28',\n",
        " 'SpO29',\n",
        " 'SpO211',\n",
        " 'SpO212',\n",
        " 'SpO213',\n",
        " 'SpO214',\n",
        " 'SpO215',\n",
        " 'SpO216',\n",
        " 'SpO218',\n",
        " 'SpO219',\n",
        " 'SpO220',\n",
        " 'SpO221',\n",
        " 'SpO20_current_stay',\n",
        " 'SpO22_current_stay',\n",
        " 'SpO23_current_stay',\n",
        " 'SpO24_current_stay',\n",
        " 'SpO25_current_stay',\n",
        " 'SpO26_current_stay',\n",
        " 'SpO28_current_stay',\n",
        " 'SpO212_current_stay',\n",
        " 'SpO213_current_stay',\n",
        " 'SpO214_current_stay',\n",
        " 'SpO216_current_stay',\n",
        " 'SpO218_current_stay',\n",
        " 'SpO219_current_stay',\n",
        " 'SpO220_current_stay',\n",
        " 'SpO221_current_stay',\n",
        " 'Systolic Blood Pressure2',\n",
        " 'Systolic Blood Pressure3',\n",
        " 'Systolic Blood Pressure4',\n",
        " 'Systolic Blood Pressure5',\n",
        " 'Systolic Blood Pressure6',\n",
        " 'Systolic Blood Pressure7',\n",
        " 'Systolic Blood Pressure8',\n",
        " 'Systolic Blood Pressure9',\n",
        " 'Systolic Blood Pressure11',\n",
        " 'Systolic Blood Pressure12',\n",
        " 'Systolic Blood Pressure13',\n",
        " 'Systolic Blood Pressure14',\n",
        " 'Systolic Blood Pressure15',\n",
        " 'Systolic Blood Pressure16',\n",
        " 'Systolic Blood Pressure18',\n",
        " 'Systolic Blood Pressure19',\n",
        " 'Systolic Blood Pressure20',\n",
        " 'Systolic Blood Pressure21',\n",
        " 'Systolic Blood Pressure2_current_stay',\n",
        " 'Systolic Blood Pressure3_current_stay',\n",
        " 'Systolic Blood Pressure4_current_stay',\n",
        " 'Systolic Blood Pressure5_current_stay',\n",
        " 'Systolic Blood Pressure6_current_stay',\n",
        " 'Systolic Blood Pressure8_current_stay',\n",
        " 'Systolic Blood Pressure12_current_stay',\n",
        " 'Systolic Blood Pressure13_current_stay',\n",
        " 'Systolic Blood Pressure14_current_stay',\n",
        " 'Systolic Blood Pressure16_current_stay',\n",
        " 'Systolic Blood Pressure18_current_stay',\n",
        " 'Systolic Blood Pressure19_current_stay',\n",
        " 'Systolic Blood Pressure20_current_stay',\n",
        " 'Systolic Blood Pressure21_current_stay',\n",
        " 'Temperature0',\n",
        " 'Temperature2',\n",
        " 'Temperature3',\n",
        " 'Temperature4',\n",
        " 'Temperature5',\n",
        " 'Temperature6',\n",
        " 'Temperature7',\n",
        " 'Temperature8',\n",
        " 'Temperature9',\n",
        " 'Temperature11',\n",
        " 'Temperature12',\n",
        " 'Temperature13',\n",
        " 'Temperature14',\n",
        " 'Temperature15',\n",
        " 'Temperature16',\n",
        " 'Temperature18',\n",
        " 'Temperature19',\n",
        " 'Temperature20',\n",
        " 'Temperature21',\n",
        " 'Temperature0_current_stay',\n",
        " 'Temperature2_current_stay',\n",
        " 'Temperature3_current_stay',\n",
        " 'Temperature4_current_stay',\n",
        " 'Temperature5_current_stay',\n",
        " 'Temperature6_current_stay',\n",
        " 'Temperature8_current_stay',\n",
        " 'Temperature12_current_stay',\n",
        " 'Temperature13_current_stay',\n",
        " 'Temperature14_current_stay',\n",
        " 'Temperature16_current_stay',\n",
        " 'Temperature18_current_stay',\n",
        " 'Temperature19_current_stay',\n",
        " 'Temperature20_current_stay',\n",
        " 'Temperature21_current_stay',\n",
        " 'Diastolic Blood Pressure2_difference',\n",
        " 'Diastolic Blood Pressure3_difference',\n",
        " 'Diastolic Blood Pressure4_difference',\n",
        " 'Diastolic Blood Pressure5_difference',\n",
        " 'Diastolic Blood Pressure6_difference',\n",
        " 'Diastolic Blood Pressure7_difference',\n",
        " 'Diastolic Blood Pressure8_difference',\n",
        " 'Diastolic Blood Pressure9_difference',\n",
        " 'Diastolic Blood Pressure11_difference',\n",
        " 'Diastolic Blood Pressure12_difference',\n",
        " 'Diastolic Blood Pressure13_difference',\n",
        " 'Diastolic Blood Pressure14_difference',\n",
        " 'Diastolic Blood Pressure15_difference',\n",
        " 'Diastolic Blood Pressure16_difference',\n",
        " 'Diastolic Blood Pressure18_difference',\n",
        " 'Diastolic Blood Pressure19_difference',\n",
        " 'Diastolic Blood Pressure20_difference',\n",
        " 'Diastolic Blood Pressure21_difference',\n",
        " 'Diastolic Blood Pressure2_current_stay_difference',\n",
        " 'Diastolic Blood Pressure3_current_stay_difference',\n",
        " 'Diastolic Blood Pressure4_current_stay_difference',\n",
        " 'Diastolic Blood Pressure5_current_stay_difference',\n",
        " 'Diastolic Blood Pressure6_current_stay_difference',\n",
        " 'Diastolic Blood Pressure8_current_stay_difference',\n",
        " 'Diastolic Blood Pressure12_current_stay_difference',\n",
        " 'Diastolic Blood Pressure13_current_stay_difference',\n",
        " 'Diastolic Blood Pressure14_current_stay_difference',\n",
        " 'Diastolic Blood Pressure16_current_stay_difference',\n",
        " 'Diastolic Blood Pressure18_current_stay_difference',\n",
        " 'Diastolic Blood Pressure19_current_stay_difference',\n",
        " 'Diastolic Blood Pressure21_current_stay_difference',\n",
        " 'Glasgow Coma Score0_difference',\n",
        " 'Glasgow Coma Score1_difference',\n",
        " 'Glasgow Coma Score2_difference',\n",
        " 'Glasgow Coma Score3_difference',\n",
        " 'Glasgow Coma Score4_difference',\n",
        " 'Glasgow Coma Score5_difference',\n",
        " 'Glasgow Coma Score6_difference',\n",
        " 'Glasgow Coma Score7_difference',\n",
        " 'Glasgow Coma Score8_difference',\n",
        " 'Glasgow Coma Score9_difference',\n",
        " 'Glasgow Coma Score10_difference',\n",
        " 'Glasgow Coma Score11_difference',\n",
        " 'Glasgow Coma Score12_difference',\n",
        " 'Glasgow Coma Score13_difference',\n",
        " 'Glasgow Coma Score14_difference',\n",
        " 'Glasgow Coma Score15_difference',\n",
        " 'Glasgow Coma Score16_difference',\n",
        " 'Glasgow Coma Score17_difference',\n",
        " 'Glasgow Coma Score18_difference',\n",
        " 'Glasgow Coma Score19_difference',\n",
        " 'Glasgow Coma Score20_difference',\n",
        " 'Glasgow Coma Score21_difference',\n",
        " 'Glasgow Coma Score0_current_stay_difference',\n",
        " 'Glasgow Coma Score1_current_stay_difference',\n",
        " 'Glasgow Coma Score2_current_stay_difference',\n",
        " 'Glasgow Coma Score3_current_stay_difference',\n",
        " 'Glasgow Coma Score4_current_stay_difference',\n",
        " 'Glasgow Coma Score5_current_stay_difference',\n",
        " 'Glasgow Coma Score6_current_stay_difference',\n",
        " 'Glasgow Coma Score7_current_stay_difference',\n",
        " 'Glasgow Coma Score8_current_stay_difference',\n",
        " 'Glasgow Coma Score10_current_stay_difference',\n",
        " 'Glasgow Coma Score11_current_stay_difference',\n",
        " 'Glasgow Coma Score12_current_stay_difference',\n",
        " 'Glasgow Coma Score13_current_stay_difference',\n",
        " 'Glasgow Coma Score14_current_stay_difference',\n",
        " 'Glasgow Coma Score15_current_stay_difference',\n",
        " 'Glasgow Coma Score16_current_stay_difference',\n",
        " 'Glasgow Coma Score17_current_stay_difference',\n",
        " 'Glasgow Coma Score18_current_stay_difference',\n",
        " 'Glasgow Coma Score19_current_stay_difference',\n",
        " 'Glasgow Coma Score20_current_stay_difference',\n",
        " 'Glasgow Coma Score21_current_stay_difference',\n",
        " 'Heart Rate2_difference',\n",
        " 'Heart Rate3_difference',\n",
        " 'Heart Rate4_difference',\n",
        " 'Heart Rate5_difference',\n",
        " 'Heart Rate6_difference',\n",
        " 'Heart Rate7_difference',\n",
        " 'Heart Rate8_difference',\n",
        " 'Heart Rate9_difference',\n",
        " 'Heart Rate11_difference',\n",
        " 'Heart Rate12_difference',\n",
        " 'Heart Rate13_difference',\n",
        " 'Heart Rate14_difference',\n",
        " 'Heart Rate15_difference',\n",
        " 'Heart Rate16_difference',\n",
        " 'Heart Rate18_difference',\n",
        " 'Heart Rate19_difference',\n",
        " 'Heart Rate20_difference',\n",
        " 'Heart Rate21_difference',\n",
        " 'Heart Rate2_current_stay_difference',\n",
        " 'Heart Rate3_current_stay_difference',\n",
        " 'Heart Rate4_current_stay_difference',\n",
        " 'Heart Rate5_current_stay_difference',\n",
        " 'Heart Rate6_current_stay_difference',\n",
        " 'Heart Rate8_current_stay_difference',\n",
        " 'Heart Rate12_current_stay_difference',\n",
        " 'Heart Rate13_current_stay_difference',\n",
        " 'Heart Rate14_current_stay_difference',\n",
        " 'Heart Rate16_current_stay_difference',\n",
        " 'Heart Rate18_current_stay_difference',\n",
        " 'Heart Rate19_current_stay_difference',\n",
        " 'Heart Rate21_current_stay_difference',\n",
        " 'Mean Arterial Pressure2_difference',\n",
        " 'Mean Arterial Pressure3_difference',\n",
        " 'Mean Arterial Pressure4_difference',\n",
        " 'Mean Arterial Pressure5_difference',\n",
        " 'Mean Arterial Pressure6_difference',\n",
        " 'Mean Arterial Pressure7_difference',\n",
        " 'Mean Arterial Pressure8_difference',\n",
        " 'Mean Arterial Pressure9_difference',\n",
        " 'Mean Arterial Pressure11_difference',\n",
        " 'Mean Arterial Pressure12_difference',\n",
        " 'Mean Arterial Pressure13_difference',\n",
        " 'Mean Arterial Pressure14_difference',\n",
        " 'Mean Arterial Pressure15_difference',\n",
        " 'Mean Arterial Pressure16_difference',\n",
        " 'Mean Arterial Pressure18_difference',\n",
        " 'Mean Arterial Pressure19_difference',\n",
        " 'Mean Arterial Pressure20_difference',\n",
        " 'Mean Arterial Pressure21_difference',\n",
        " 'Mean Arterial Pressure2_current_stay_difference',\n",
        " 'Mean Arterial Pressure3_current_stay_difference',\n",
        " 'Mean Arterial Pressure4_current_stay_difference',\n",
        " 'Mean Arterial Pressure5_current_stay_difference',\n",
        " 'Mean Arterial Pressure6_current_stay_difference',\n",
        " 'Mean Arterial Pressure8_current_stay_difference',\n",
        " 'Mean Arterial Pressure12_current_stay_difference',\n",
        " 'Mean Arterial Pressure13_current_stay_difference',\n",
        " 'Mean Arterial Pressure14_current_stay_difference',\n",
        " 'Mean Arterial Pressure16_current_stay_difference',\n",
        " 'Mean Arterial Pressure18_current_stay_difference',\n",
        " 'Mean Arterial Pressure19_current_stay_difference',\n",
        " 'Mean Arterial Pressure21_current_stay_difference',\n",
        " 'NEWS Conscious Level Score0_difference',\n",
        " 'NEWS Conscious Level Score1_difference',\n",
        " 'NEWS Conscious Level Score2_difference',\n",
        " 'NEWS Conscious Level Score3_difference',\n",
        " 'NEWS Conscious Level Score4_difference',\n",
        " 'NEWS Conscious Level Score5_difference',\n",
        " 'NEWS Conscious Level Score6_difference',\n",
        " 'NEWS Conscious Level Score7_difference',\n",
        " 'NEWS Conscious Level Score8_difference',\n",
        " 'NEWS Conscious Level Score9_difference',\n",
        " 'NEWS Conscious Level Score10_difference',\n",
        " 'NEWS Conscious Level Score11_difference',\n",
        " 'NEWS Conscious Level Score12_difference',\n",
        " 'NEWS Conscious Level Score13_difference',\n",
        " 'NEWS Conscious Level Score14_difference',\n",
        " 'NEWS Conscious Level Score15_difference',\n",
        " 'NEWS Conscious Level Score16_difference',\n",
        " 'NEWS Conscious Level Score17_difference',\n",
        " 'NEWS Conscious Level Score18_difference',\n",
        " 'NEWS Conscious Level Score19_difference',\n",
        " 'NEWS Conscious Level Score20_difference',\n",
        " 'NEWS Conscious Level Score21_difference',\n",
        " 'NEWS Conscious Level Score0_current_stay_difference',\n",
        " 'NEWS Conscious Level Score1_current_stay_difference',\n",
        " 'NEWS Conscious Level Score2_current_stay_difference',\n",
        " 'NEWS Conscious Level Score3_current_stay_difference',\n",
        " 'NEWS Conscious Level Score4_current_stay_difference',\n",
        " 'NEWS Conscious Level Score5_current_stay_difference',\n",
        " 'NEWS Conscious Level Score6_current_stay_difference',\n",
        " 'NEWS Conscious Level Score7_current_stay_difference',\n",
        " 'NEWS Conscious Level Score8_current_stay_difference',\n",
        " 'NEWS Conscious Level Score9_current_stay_difference',\n",
        " 'NEWS Conscious Level Score10_current_stay_difference',\n",
        " 'NEWS Conscious Level Score11_current_stay_difference',\n",
        " 'NEWS Conscious Level Score12_current_stay_difference',\n",
        " 'NEWS Conscious Level Score13_current_stay_difference',\n",
        " 'NEWS Conscious Level Score14_current_stay_difference',\n",
        " 'NEWS Conscious Level Score15_current_stay_difference',\n",
        " 'NEWS Conscious Level Score16_current_stay_difference',\n",
        " 'NEWS Conscious Level Score17_current_stay_difference',\n",
        " 'NEWS Conscious Level Score18_current_stay_difference',\n",
        " 'NEWS Conscious Level Score19_current_stay_difference',\n",
        " 'NEWS Conscious Level Score20_current_stay_difference',\n",
        " 'NEWS Conscious Level Score21_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc0_difference',\n",
        " 'NEWS Supplemental Oxygen Calc1_difference',\n",
        " 'NEWS Supplemental Oxygen Calc2_difference',\n",
        " 'NEWS Supplemental Oxygen Calc3_difference',\n",
        " 'NEWS Supplemental Oxygen Calc4_difference',\n",
        " 'NEWS Supplemental Oxygen Calc5_difference',\n",
        " 'NEWS Supplemental Oxygen Calc6_difference',\n",
        " 'NEWS Supplemental Oxygen Calc7_difference',\n",
        " 'NEWS Supplemental Oxygen Calc8_difference',\n",
        " 'NEWS Supplemental Oxygen Calc9_difference',\n",
        " 'NEWS Supplemental Oxygen Calc10_difference',\n",
        " 'NEWS Supplemental Oxygen Calc11_difference',\n",
        " 'NEWS Supplemental Oxygen Calc12_difference',\n",
        " 'NEWS Supplemental Oxygen Calc13_difference',\n",
        " 'NEWS Supplemental Oxygen Calc14_difference',\n",
        " 'NEWS Supplemental Oxygen Calc15_difference',\n",
        " 'NEWS Supplemental Oxygen Calc16_difference',\n",
        " 'NEWS Supplemental Oxygen Calc17_difference',\n",
        " 'NEWS Supplemental Oxygen Calc18_difference',\n",
        " 'NEWS Supplemental Oxygen Calc19_difference',\n",
        " 'NEWS Supplemental Oxygen Calc20_difference',\n",
        " 'NEWS Supplemental Oxygen Calc21_difference',\n",
        " 'NEWS Supplemental Oxygen Calc0_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc1_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc2_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc3_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc4_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc5_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc6_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc7_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc8_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc10_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc12_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc13_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc14_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc15_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc16_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc18_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc19_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc20_current_stay_difference',\n",
        " 'NEWS Supplemental Oxygen Calc21_current_stay_difference',\n",
        " 'Respiratory Rate2_difference',\n",
        " 'Respiratory Rate3_difference',\n",
        " 'Respiratory Rate4_difference',\n",
        " 'Respiratory Rate5_difference',\n",
        " 'Respiratory Rate6_difference',\n",
        " 'Respiratory Rate7_difference',\n",
        " 'Respiratory Rate8_difference',\n",
        " 'Respiratory Rate9_difference',\n",
        " 'Respiratory Rate11_difference',\n",
        " 'Respiratory Rate12_difference',\n",
        " 'Respiratory Rate13_difference',\n",
        " 'Respiratory Rate14_difference',\n",
        " 'Respiratory Rate15_difference',\n",
        " 'Respiratory Rate16_difference',\n",
        " 'Respiratory Rate18_difference',\n",
        " 'Respiratory Rate19_difference',\n",
        " 'Respiratory Rate20_difference',\n",
        " 'Respiratory Rate21_difference',\n",
        " 'Respiratory Rate0_current_stay_difference',\n",
        " 'Respiratory Rate2_current_stay_difference',\n",
        " 'Respiratory Rate3_current_stay_difference',\n",
        " 'Respiratory Rate4_current_stay_difference',\n",
        " 'Respiratory Rate5_current_stay_difference',\n",
        " 'Respiratory Rate6_current_stay_difference',\n",
        " 'Respiratory Rate8_current_stay_difference',\n",
        " 'Respiratory Rate12_current_stay_difference',\n",
        " 'Respiratory Rate13_current_stay_difference',\n",
        " 'Respiratory Rate14_current_stay_difference',\n",
        " 'Respiratory Rate16_current_stay_difference',\n",
        " 'Respiratory Rate18_current_stay_difference',\n",
        " 'Respiratory Rate19_current_stay_difference',\n",
        " 'Respiratory Rate21_current_stay_difference',\n",
        " 'SpO22_difference',\n",
        " 'SpO23_difference',\n",
        " 'SpO24_difference',\n",
        " 'SpO25_difference',\n",
        " 'SpO26_difference',\n",
        " 'SpO27_difference',\n",
        " 'SpO28_difference',\n",
        " 'SpO29_difference',\n",
        " 'SpO211_difference',\n",
        " 'SpO212_difference',\n",
        " 'SpO213_difference',\n",
        " 'SpO214_difference',\n",
        " 'SpO215_difference',\n",
        " 'SpO216_difference',\n",
        " 'SpO218_difference',\n",
        " 'SpO219_difference',\n",
        " 'SpO220_difference',\n",
        " 'SpO221_difference',\n",
        " 'SpO22_current_stay_difference',\n",
        " 'SpO23_current_stay_difference',\n",
        " 'SpO24_current_stay_difference',\n",
        " 'SpO25_current_stay_difference',\n",
        " 'SpO26_current_stay_difference',\n",
        " 'SpO28_current_stay_difference',\n",
        " 'SpO212_current_stay_difference',\n",
        " 'SpO213_current_stay_difference',\n",
        " 'SpO214_current_stay_difference',\n",
        " 'SpO216_current_stay_difference',\n",
        " 'SpO218_current_stay_difference',\n",
        " 'SpO219_current_stay_difference',\n",
        " 'SpO221_current_stay_difference',\n",
        " 'Systolic Blood Pressure2_difference',\n",
        " 'Systolic Blood Pressure3_difference',\n",
        " 'Systolic Blood Pressure4_difference',\n",
        " 'Systolic Blood Pressure5_difference',\n",
        " 'Systolic Blood Pressure6_difference',\n",
        " 'Systolic Blood Pressure7_difference',\n",
        " 'Systolic Blood Pressure8_difference',\n",
        " 'Systolic Blood Pressure9_difference',\n",
        " 'Systolic Blood Pressure11_difference',\n",
        " 'Systolic Blood Pressure12_difference',\n",
        " 'Systolic Blood Pressure13_difference',\n",
        " 'Systolic Blood Pressure14_difference',\n",
        " 'Systolic Blood Pressure15_difference',\n",
        " 'Systolic Blood Pressure16_difference',\n",
        " 'Systolic Blood Pressure18_difference',\n",
        " 'Systolic Blood Pressure19_difference',\n",
        " 'Systolic Blood Pressure20_difference',\n",
        " 'Systolic Blood Pressure21_difference',\n",
        " 'Systolic Blood Pressure2_current_stay_difference',\n",
        " 'Systolic Blood Pressure3_current_stay_difference',\n",
        " 'Systolic Blood Pressure4_current_stay_difference',\n",
        " 'Systolic Blood Pressure5_current_stay_difference',\n",
        " 'Systolic Blood Pressure6_current_stay_difference',\n",
        " 'Systolic Blood Pressure8_current_stay_difference',\n",
        " 'Systolic Blood Pressure12_current_stay_difference',\n",
        " 'Systolic Blood Pressure13_current_stay_difference',\n",
        " 'Systolic Blood Pressure14_current_stay_difference',\n",
        " 'Systolic Blood Pressure16_current_stay_difference',\n",
        " 'Systolic Blood Pressure18_current_stay_difference',\n",
        " 'Systolic Blood Pressure19_current_stay_difference',\n",
        " 'Systolic Blood Pressure21_current_stay_difference',\n",
        " 'Temperature2_difference',\n",
        " 'Temperature3_difference',\n",
        " 'Temperature4_difference',\n",
        " 'Temperature5_difference',\n",
        " 'Temperature6_difference',\n",
        " 'Temperature7_difference',\n",
        " 'Temperature8_difference',\n",
        " 'Temperature9_difference',\n",
        " 'Temperature11_difference',\n",
        " 'Temperature12_difference',\n",
        " 'Temperature13_difference',\n",
        " 'Temperature14_difference',\n",
        " 'Temperature15_difference',\n",
        " 'Temperature16_difference',\n",
        " 'Temperature18_difference',\n",
        " 'Temperature19_difference',\n",
        " 'Temperature20_difference',\n",
        " 'Temperature21_difference',\n",
        " 'Temperature2_current_stay_difference',\n",
        " 'Temperature3_current_stay_difference',\n",
        " 'Temperature4_current_stay_difference',\n",
        " 'Temperature5_current_stay_difference',\n",
        " 'Temperature6_current_stay_difference',\n",
        " 'Temperature8_current_stay_difference',\n",
        " 'Temperature12_current_stay_difference',\n",
        " 'Temperature13_current_stay_difference',\n",
        " 'Temperature14_current_stay_difference',\n",
        " 'Temperature16_current_stay_difference',\n",
        " 'Temperature18_current_stay_difference',\n",
        " 'Temperature19_current_stay_difference',\n",
        " 'Temperature21_current_stay_difference']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1711640775962
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    batch_prediction_list = []\n",
        "    batch_label_list = []\n",
        "\n",
        "    # use the with torch.no_grad() block to ensure no gradients are calculated within the bloc\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, sample in enumerate(tqdm(dataloader)):\n",
        "            labels = sample[\"labels\"]\n",
        "            features = sample[\"features\"]\n",
        "            batch_mask = sample[\"mask\"]\n",
        "            features = [data.float() for data in features]\n",
        "            features = [data.to(device=device) for data in features]\n",
        "            labels = labels.float()\n",
        "            labels = labels.to(device=device)\n",
        "            batch_mask = batch_mask.to(device)\n",
        "\n",
        "            # Run model\n",
        "            output = model(features, batch_mask)\n",
        "\n",
        "            # Generate loss\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Get predictions for performance calc - masking outputs and labels\n",
        "            sig = torch.nn.Sigmoid()\n",
        "            output = sig(output)  \n",
        "            np_predictions = output.cpu().detach().numpy()\n",
        "            np_labels = labels.cpu().detach().numpy()\n",
        "\n",
        "            np_predictions = np_predictions.squeeze()\n",
        "            np_labels = np_labels.squeeze()\n",
        "\n",
        "            np_predictions = np_predictions.flatten()\n",
        "            np_labels = np_labels.flatten()\n",
        "            \n",
        "            # Create list\n",
        "            for x in np_predictions:\n",
        "                batch_prediction_list.append(x)\n",
        "            for x in np_labels:\n",
        "                batch_label_list.append(x)\n",
        "\n",
        "        final_predictions = np.array(batch_prediction_list)\n",
        "\n",
        "        final_labels = np.array(batch_label_list)\n",
        "\n",
        "        try:\n",
        "            auroc = roc_auc_score(final_labels, final_predictions)\n",
        "        except:\n",
        "            auroc = np.nan\n",
        "        \n",
        "        try:\n",
        "            final_loss = epoch_loss / len(dataloader)\n",
        "        except:\n",
        "            final_loss = np.nan\n",
        "\n",
        "        return final_loss, auroc, final_predictions, final_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1711640776309
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class SetTransformer(nn.Module):\n",
        "    def __init__(self, dim_input, num_outputs, dim_output,\n",
        "            num_inds=36, dim_hidden=160, num_heads=4, ln=False):\n",
        "        super(SetTransformer, self).__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "                ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
        "                ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln))\n",
        "        self.isab = ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln)\n",
        "        self.pma = PMA(dim_hidden, num_heads, num_outputs, ln=ln)\n",
        "        self.dec = nn.Sequential(\n",
        "                #SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
        "                #SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
        "                nn.Linear(dim_hidden, dim_output))\n",
        "\n",
        "    def forward(self, X, batch_mask):\n",
        "        x = self.isab(X, batch_mask)\n",
        "        x = self.pma(x, batch_mask)\n",
        "        return self.dec(x)#, x\n",
        "\n",
        "class MAB0(nn.Module):\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
        "        super(MAB0, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
        "        if ln:\n",
        "            self.ln0 = nn.LayerNorm(dim_V)\n",
        "            self.ln1 = nn.LayerNorm(dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "\n",
        "    def forward(self, Q, K, mask):\n",
        "        Q = self.fc_q(Q)\n",
        "        K, V = self.fc_k(K), self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
        "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
        "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
        "\n",
        "        # Create new variable for softmax\n",
        "        WB_ = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
        "        # Exspand mask dimensions to align\n",
        "        mask = mask.unsqueeze(1).repeat(self.num_heads, Q.shape[1], 1)\n",
        "        # Mask for softmax\n",
        "        WB_[~mask] = float('-inf')\n",
        "        \n",
        "        A = torch.softmax(WB_, 2)\n",
        "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
        "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
        "        O = O + F.relu(self.fc_o(O))\n",
        "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
        "        return O\n",
        "\n",
        "class MAB(nn.Module):\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
        "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
        "        if ln:\n",
        "            self.ln0 = nn.LayerNorm(dim_V)\n",
        "            self.ln1 = nn.LayerNorm(dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "\n",
        "    def forward(self, Q, K):\n",
        "        Q = self.fc_q(Q)\n",
        "        K, V = self.fc_k(K), self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
        "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
        "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
        "\n",
        "        A = torch.softmax(Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V), 2)\n",
        "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
        "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
        "        O = O + F.relu(self.fc_o(O))\n",
        "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
        "        return O\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
        "        super(SAB, self).__init__()\n",
        "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.mab(X, X)\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "        self.mab0 = MAB0(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
        "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X, mask):\n",
        "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, mask)\n",
        "        return self.mab1(X, H)\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
        "        super(PMA, self).__init__()\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "        self.mab = MAB0(dim, dim, dim, num_heads, ln=ln)\n",
        "\n",
        "    def forward(self, X, mask):\n",
        "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1711640776665
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class Initial_vitals_model(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_dim),\n",
        "            nn.Linear(input_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hid_dim, output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.layers(x)\n",
        "\n",
        "        return x1\n",
        "\n",
        "\n",
        "class Chronic_switch_model(nn.Module):\n",
        "    def __init__(self, \n",
        "    final_input_dim, \n",
        "    final_output_dim, \n",
        "    final_hid_dim, \n",
        "    final_hid_dim2,\n",
        "    demographics_input_dim,\n",
        "    demographics_output_dim,\n",
        "    vital_input_dim, \n",
        "    vital_hid_dim, \n",
        "    vital_output_dim, \n",
        "    dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.final_layers = nn.Sequential(\n",
        "            nn.Linear(final_input_dim, final_hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(final_hid_dim, final_hid_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(final_hid_dim2, final_output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        self.vital_model = Initial_vitals_model(vital_input_dim, vital_output_dim, vital_hid_dim, dropout)\n",
        "\n",
        "        self.set_transformer = SetTransformer(dim_input=128, num_outputs=1, dim_output=128, num_inds=32, dim_hidden=160, num_heads=4, ln=False)\n",
        "\n",
        "        # Embedding for demographics (passing feature directly)\n",
        "        self.demographics = nn.Linear(demographics_input_dim, demographics_output_dim)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, batch_mask: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # Directly pass demographics feature to embedding\n",
        "        demographics = self.demographics(inputs[1])\n",
        "\n",
        "        # Pass other inputs through initial models\n",
        "        vital_embeddings = self.vital_model(inputs[0])\n",
        "        comorbidity_embeddings = self.set_transformer(inputs[2], batch_mask)\n",
        "        comorbidity_embeddings = torch.squeeze(comorbidity_embeddings)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        concatenated_embeddings = torch.cat([demographics, vital_embeddings, comorbidity_embeddings], dim=1)\n",
        "\n",
        "        # Pass through final layers\n",
        "        output = self.final_layers(concatenated_embeddings)\n",
        "\n",
        "        return output\n",
        "    \n",
        "    def latent_representation(self, inputs: torch.Tensor, batch_mask: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # Directly pass demographics feature to embedding\n",
        "        demographics = self.demographics(inputs[1])\n",
        "\n",
        "        # Pass other inputs through initial models\n",
        "        vital_embeddings = self.vital_model(inputs[0])\n",
        "        comorbidity_embeddings = self.set_transformer(inputs[2], batch_mask)\n",
        "        comorbidity_embeddings = torch.squeeze(comorbidity_embeddings)\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        concatenated_embeddings = torch.cat([demographics, vital_embeddings, comorbidity_embeddings], dim=1)\n",
        "        \n",
        "        return concatenated_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1711640777001
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class MultiInputDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dfs_list, labels, comorbidites, padding_mask):\n",
        "        self.dfs_list = dfs_list\n",
        "        self.labels = labels\n",
        "        self.padding_mask = padding_mask\n",
        "        self.comorbidites = comorbidites\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = [torch.tensor(df.iloc[idx].values) for df in self.dfs_list]\n",
        "        labels = torch.tensor(self.labels[['po_flag']].iloc[idx].to_numpy())\n",
        "        features.append(self.comorbidites[idx])\n",
        "        sample = {\"labels\": labels, \"features\": features, \"mask\":self.padding_mask[idx]}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1711640777250
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def analyze_results_fun(cv_test_results):\n",
        "    # Assign results\n",
        "    test_auroc_results, test_accuracy_results,test_balanced_accuracy_results,test_recall_results,test_precision_results,test_f1_results,test_auprc_results,test_cm_results, test_tpr_results, test_fpr_results = [cv_test_results[i] for i in range(len(cv_test_results))]\n",
        "    print('mean test_auroc:', np.array(test_auroc_results).mean())\n",
        "    print('std test_auroc:', np.array(test_auroc_results).std())\n",
        "    print('test_auroc 2.5th percentile:', max(0, np.percentile(test_auroc_results, 2.5)))\n",
        "    print('test_auroc 97.5th percentile:', min(1, np.percentile(test_auroc_results, 97.5)))\n",
        "    print('mean test_accuracy:', np.array(test_accuracy_results).mean())\n",
        "    print('std test_accuracy:', np.array(test_accuracy_results).std())\n",
        "    print('test_accuracy 2.5th percentile:', max(0, np.percentile(test_accuracy_results, 2.5)))\n",
        "    print('test_accuracy 97.5th percentile:', min(1, np.percentile(test_accuracy_results, 97.5)))\n",
        "    print('mean test_balanced_accuracy:', np.array(test_balanced_accuracy_results).mean())\n",
        "    print('std test_balanced_accuracy:', np.array(test_balanced_accuracy_results).std())\n",
        "    print('test_balanced_accuracy 2.5th percentile:', max(0, np.percentile(test_balanced_accuracy_results, 2.5)))\n",
        "    print('test_balanced_accuracy 97.5th percentile:', min(1, np.percentile(test_balanced_accuracy_results, 97.5)))\n",
        "    print('mean test_recall:', np.array(test_recall_results).mean())\n",
        "    print('std test_recall:', np.array(test_recall_results).std())\n",
        "    print('test_recall 2.5th percentile:', max(0, np.percentile(test_recall_results, 2.5)))\n",
        "    print('test_recall 97.5th percentile:', min(1, np.percentile(test_recall_results, 97.5)))\n",
        "    print('mean test_precision:', np.array(test_precision_results).mean())\n",
        "    print('std test_precision:', np.array(test_precision_results).std())\n",
        "    print('test_precision 2.5th percentile:', max(0, np.percentile(test_precision_results, 2.5)))\n",
        "    print('test_precision 97.5th percentile:', min(1, np.percentile(test_precision_results, 97.5)))\n",
        "    print('mean test_f1:', np.array(test_f1_results).mean())\n",
        "    print('std test_f1:', np.array(test_f1_results).std())\n",
        "    print('test_f1 2.5th percentile:', max(0, np.percentile(test_f1_results, 2.5)))\n",
        "    print('test_f1 97.5th percentile:', min(1, np.percentile(test_f1_results, 97.5)))\n",
        "    print('mean test_auprc:', np.array(test_auprc_results).mean())\n",
        "    print('std test_auprc:', np.array(test_auprc_results).std())\n",
        "    print('test_auprc 2.5th percentile:', max(0, np.percentile(test_auprc_results, 2.5)))\n",
        "    print('test_auprc 97.5th percentile:', min(1, np.percentile(test_auprc_results, 97.5)))\n",
        "    print('mean test_tpr:', np.array(test_tpr_results).mean())\n",
        "    print('std test_tpr:', np.array(test_tpr_results).std())\n",
        "    print('test_tpr 2.5th percentile:', max(0, np.percentile(test_tpr_results, 2.5)))\n",
        "    print('test_tpr 97.5th percentile:', min(1, np.percentile(test_tpr_results, 97.5)))\n",
        "    print('mean test_fpr:', np.array(test_fpr_results).mean())\n",
        "    print('std test_fpr:', np.array(test_fpr_results).std())\n",
        "    print('test_fpr 2.5th percentile:', max(0, np.percentile(test_fpr_results, 2.5)))\n",
        "    print('test_fpr 97.5th percentile:', min(1, np.percentile(test_fpr_results, 97.5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1711640777645
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,126,583 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "\n",
        "# Hyperparameters\n",
        "final_input_dim = 268\n",
        "final_output_dim = 1\n",
        "final_hid_dim = 512\n",
        "final_hid_dim2 = 128\n",
        "demographics_input_dim = 12\n",
        "demographics_output_dim = 12\n",
        "vital_input_dim = 253\n",
        "vital_hid_dim = 512\n",
        "vital_output_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "# Define model\n",
        "model = Chronic_switch_model(\n",
        "    final_input_dim, \n",
        "    final_output_dim, \n",
        "    final_hid_dim, \n",
        "    final_hid_dim2,\n",
        "    demographics_input_dim,\n",
        "    demographics_output_dim,\n",
        "    vital_input_dim, \n",
        "    vital_hid_dim, \n",
        "    vital_output_dim, \n",
        "    dropout).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load('chronic_switch_model.pt'))\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1711640783277
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "path = r'switch_data/chronic_switch_icare_df_preprocessed_2023.csv'\n",
        "icare_df_preprocessed = pd.read_csv(path)\n",
        "\n",
        "# Import\n",
        "path = r'switch_data/chronic_switch_episodes_2023.csv'\n",
        "episodes = pd.read_csv(path)\n",
        "\n",
        "# Import\n",
        "path = r'switch_data/chronic_switch_disease_2023.csv'\n",
        "disease = pd.read_csv(path)\n",
        "\n",
        "# Import\n",
        "path = r'switch_data/chronic_switch_demographics_2023.csv'\n",
        "demographics = pd.read_csv(path)\n",
        "\n",
        "# Import\n",
        "path = r'switch_data/snomed_embedding_128d-copy.csv'\n",
        "embedding = pd.read_csv(path)\n",
        "\n",
        "# Import\n",
        "path = r'switch_data/chronic_switch_problem_dummies_2023.csv'\n",
        "problem_dummies = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1711640893451
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8355/2272838105.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  problem_dummies2['time_diff'] = (problem_dummies2['date'] - problem_dummies2['DT_TM']).abs()\n"
          ]
        }
      ],
      "source": [
        "# Merge\n",
        "problem_dummies2 = pd.merge(problem_dummies, episodes[['SUBJECT', 'SPELL_IDENTIFIER']])\n",
        "problem_dummies2 = pd.merge(icare_df_preprocessed[['SPELL_IDENTIFIER', 'date']], problem_dummies2)\n",
        "\n",
        "# Strip name \n",
        "problem_dummies2.columns = problem_dummies2.columns.str.removeprefix('PROBLEM_')\n",
        "\n",
        "# Convert the date columns to datetime objects if they are not already\n",
        "problem_dummies2['date'] = pd.to_datetime(problem_dummies2['date'])\n",
        "problem_dummies2['DT_TM'] = pd.to_datetime(problem_dummies2['DT_TM'])\n",
        "\n",
        "# Calculate the absolute time difference between 'date' and 'DT_TM'\n",
        "problem_dummies2['time_diff'] = (problem_dummies2['date'] - problem_dummies2['DT_TM']).abs()\n",
        "\n",
        "# Filter rows where 'time_diff' is not negative\n",
        "problem_dummies2 = problem_dummies2[problem_dummies2['time_diff'] >= pd.Timedelta(0)]\n",
        "\n",
        "# Sort the DataFrame by 'SPELL_IDENTIFIER' and 'time_diff'\n",
        "problem_dummies2.sort_values(by=['SPELL_IDENTIFIER', 'time_diff'], inplace=True)\n",
        "\n",
        "# Convert to str\n",
        "problem_dummies2['date'] = problem_dummies2['date'].astype(str)\n",
        "\n",
        "# Keep only the rows with the smallest time difference for each 'SPELL_IDENTIFIER'\n",
        "problem_dummies2 = problem_dummies2.groupby(['SPELL_IDENTIFIER', 'date']).first().reset_index()\n",
        "\n",
        "### REASON the problem_dummies2 is shorter is not because it is missing some dates! \n",
        "### It is because in the final data we have some dates repeated for a spesfic spell \n",
        "### if they were admited in between 6am and 12pm the 12hour prediction is done at 6am \n",
        "### of the first day, then 48 done at 6am the next day, then the next prediction done \n",
        "### at 12pm that day causing there to be two prediction for that day...phew\n",
        "### So just get a set of co-morbidities for each spell and merge \n",
        "\n",
        "# Drop the 'time_diff' column as it's no longer needed\n",
        "problem_dummies2.drop(columns=['time_diff', 'SUBJECT', 'DT_TM', 'new_subject', 'date'], inplace=True)\n",
        "\n",
        "# Drop duplicates\n",
        "problem_dummies2.drop_duplicates(inplace=True)\n",
        "\n",
        "# Some still got through by having their co-morbidid diagnosis updated during their stay \n",
        "# In this case we just use the frst one throughout and remove the others \n",
        "# Drop duplicates\n",
        "problem_dummies2.drop_duplicates(subset=['SPELL_IDENTIFIER'], keep='first', inplace=True)\n",
        "\n",
        "# Filter for features\n",
        "X_data = icare_df_preprocessed.drop(columns=['SPELL_IDENTIFIER', 'po_flag'])\n",
        "X_data = X_data.drop(columns=columns_to_drop)\n",
        "model_data = pd.concat([icare_df_preprocessed[['SPELL_IDENTIFIER', 'po_flag']], X_data], axis=1)\n",
        "# Merge\n",
        "demographics = pd.merge(demographics, episodes[['SUBJECT', 'SPELL_IDENTIFIER']])\n",
        "demographics.drop(columns=['SUBJECT'], inplace=True)\n",
        "model_data = pd.merge(model_data, demographics, how='left')\n",
        "model_data = pd.merge(model_data, disease, how='left')\n",
        "# Drop \n",
        "model_data = model_data.drop(columns=['date', 'ROUTE', '24_hour_flag', '48_hour_flag', 'iv_treatment_length'])\n",
        "# fillna\n",
        "model_data['AGE'] = model_data['AGE'].fillna(-1)\n",
        "model_data['IMDDECIL'] = model_data['IMDDECIL'].fillna(-1)\n",
        "model_data = model_data.fillna(0)\n",
        "# Merge co-morbidites\n",
        "model_data = pd.merge(model_data, problem_dummies2, how='left')\n",
        "# Rename\n",
        "model_data.rename(columns={'SPELL_IDENTIFIER': 'stay_id'}, inplace=True)\n",
        "# Random shuffle\n",
        "stays = model_data['stay_id'].unique()\n",
        "random.Random(5).shuffle(stays)\n",
        "model_data = model_data.set_index(\"stay_id\").loc[stays].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1711640903533
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on set_transformer_processing_fun...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Split up dfs\n",
        "vitals_data = model_data.iloc[:,2:255]\n",
        "demographics_data = model_data.iloc[:,255:267]\n",
        "comorbidity_data = model_data.iloc[:, 267:]\n",
        "\n",
        "# Get labels\n",
        "labels = model_data[['po_flag']]\n",
        "\n",
        "# Preprocess comorbidity data\n",
        "print('Working on set_transformer_processing_fun...')\n",
        "comorbidity_data, comorbidity_mask = set_transformer_processing_fun(comorbidity_data, embedding)\n",
        "print('Done!')\n",
        "\n",
        "# Loss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define dataloaders\n",
        "batch_size=512\n",
        "dataset =  MultiInputDataset([vitals_data, demographics_data], labels, comorbidity_data, comorbidity_mask)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1711640910009
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def final_threshold_fun(predictions, bound=0.7373848): # Set threshold\n",
        "    new_predictions = [1 if a_ >= bound else 0 for a_ in predictions]\n",
        "    return new_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1710865492459
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 6/6 [00:03<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC result: 0.772141651875208\n",
            "new_predictions:\n",
            "AUROC: 0.7032650887567642\n",
            "Accuracy: 0.6985401459854015\n",
            "balanced_accuracy_score: 0.7032650887567643\n",
            "Recall: 0.6094510076441974\n",
            "Precision: 0.768624014022787\n",
            "F1: 0.6798449612403101\n",
            "AUPRC: 0.6735481688968101\n",
            "CM: [[1037  264]\n",
            " [ 562  877]]\n",
            "TPR: 0.6094510076441974\n",
            "FPR: 0.20292083013066872\n"
          ]
        }
      ],
      "source": [
        "loss, auroc, test_predictions, labels_out = evaluate(model, dataloader, criterion)\n",
        "\n",
        "new_predictions = final_threshold_fun(test_predictions)\n",
        "\n",
        "print('AUROC result:', auroc)\n",
        "\n",
        "# Lower bound\n",
        "print('new_predictions:')\n",
        "print('AUROC:', roc_auc_score(labels_out, new_predictions))\n",
        "print('Accuracy:', accuracy_score(labels_out, new_predictions))\n",
        "print('balanced_accuracy_score:', balanced_accuracy_score(labels_out, new_predictions))\n",
        "print('Recall:', recall_score(labels_out, new_predictions))\n",
        "print('Precision:', precision_score(labels_out, new_predictions))\n",
        "print('F1:', f1_score(labels_out, new_predictions))\n",
        "print('AUPRC:', average_precision_score(labels_out, new_predictions))\n",
        "test_cm = confusion_matrix(labels_out, new_predictions)\n",
        "print('CM:', test_cm)\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "test_true_positive_rate = (tp / (tp + fn))\n",
        "print('TPR:', test_true_positive_rate)\n",
        "test_false_positive_rate = (fp / (fp + tn))\n",
        "print('FPR:', test_false_positive_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Early, late, agree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1711640933007
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 6/6 [00:04<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUROC result: 0.772141651875208\n"
          ]
        }
      ],
      "source": [
        "loss, auroc, test_predictions, labels_out = evaluate(model, dataloader, criterion)\n",
        "\n",
        "new_predictions = final_threshold_fun(test_predictions)\n",
        "\n",
        "print('AUROC result:', auroc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1711641545225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8355/1940699546.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  problem_dummies2['time_diff'] = (problem_dummies2['date'] - problem_dummies2['DT_TM']).abs()\n"
          ]
        }
      ],
      "source": [
        "# Merge\n",
        "problem_dummies2 = pd.merge(problem_dummies, episodes[['SUBJECT', 'SPELL_IDENTIFIER']])\n",
        "problem_dummies2 = pd.merge(icare_df_preprocessed[['SPELL_IDENTIFIER', 'date']], problem_dummies2)\n",
        "\n",
        "# Strip name \n",
        "problem_dummies2.columns = problem_dummies2.columns.str.removeprefix('PROBLEM_')\n",
        "\n",
        "# Convert the date columns to datetime objects if they are not already\n",
        "problem_dummies2['date'] = pd.to_datetime(problem_dummies2['date'])\n",
        "problem_dummies2['DT_TM'] = pd.to_datetime(problem_dummies2['DT_TM'])\n",
        "\n",
        "# Calculate the absolute time difference between 'date' and 'DT_TM'\n",
        "problem_dummies2['time_diff'] = (problem_dummies2['date'] - problem_dummies2['DT_TM']).abs()\n",
        "\n",
        "# Filter rows where 'time_diff' is not negative\n",
        "problem_dummies2 = problem_dummies2[problem_dummies2['time_diff'] >= pd.Timedelta(0)]\n",
        "\n",
        "# Sort the DataFrame by 'SPELL_IDENTIFIER' and 'time_diff'\n",
        "problem_dummies2.sort_values(by=['SPELL_IDENTIFIER', 'time_diff'], inplace=True)\n",
        "\n",
        "# Convert to str\n",
        "problem_dummies2['date'] = problem_dummies2['date'].astype(str)\n",
        "\n",
        "# Keep only the rows with the smallest time difference for each 'SPELL_IDENTIFIER'\n",
        "problem_dummies2 = problem_dummies2.groupby(['SPELL_IDENTIFIER', 'date']).first().reset_index()\n",
        "\n",
        "### REASON the problem_dummies2 is shorter is not because it is missing some dates! \n",
        "### It is because in the final data we have some dates repeated for a spesfic spell \n",
        "### if they were admited in between 6am and 12pm the 12hour prediction is done at 6am \n",
        "### of the first day, then 48 done at 6am the next day, then the next prediction done \n",
        "### at 12pm that day causing there to be two prediction for that day...phew\n",
        "### So just get a set of co-morbidities for each spell and merge \n",
        "\n",
        "# Drop the 'time_diff' column as it's no longer needed\n",
        "problem_dummies2.drop(columns=['time_diff', 'SUBJECT', 'DT_TM', 'new_subject', 'date'], inplace=True)\n",
        "\n",
        "# Drop duplicates\n",
        "problem_dummies2.drop_duplicates(inplace=True)\n",
        "\n",
        "# Some still got through by having their co-morbidid diagnosis updated during their stay \n",
        "# In this case we just use the frst one throughout and remove the others \n",
        "# Drop duplicates\n",
        "problem_dummies2.drop_duplicates(subset=['SPELL_IDENTIFIER'], keep='first', inplace=True)\n",
        "\n",
        "# Filter for features\n",
        "X_data = icare_df_preprocessed.drop(columns=['SPELL_IDENTIFIER', 'po_flag'])\n",
        "X_data = X_data.drop(columns=columns_to_drop)\n",
        "model_data = pd.concat([icare_df_preprocessed[['SPELL_IDENTIFIER', 'po_flag']], X_data], axis=1)\n",
        "# Merge\n",
        "demographics = pd.merge(demographics, episodes[['SUBJECT', 'SPELL_IDENTIFIER']])\n",
        "demographics.drop(columns=['SUBJECT'], inplace=True)\n",
        "model_data = pd.merge(model_data, demographics, how='left')\n",
        "model_data = pd.merge(model_data, disease, how='left')\n",
        "# Drop \n",
        "model_data = model_data.drop(columns=['ROUTE', '24_hour_flag', '48_hour_flag'])\n",
        "# fillna\n",
        "model_data['AGE'] = model_data['AGE'].fillna(-1)\n",
        "model_data['IMDDECIL'] = model_data['IMDDECIL'].fillna(-1)\n",
        "model_data = model_data.fillna(0)\n",
        "# Merge co-morbidites\n",
        "model_data = pd.merge(model_data, problem_dummies2, how='left')\n",
        "# Rename\n",
        "model_data.rename(columns={'SPELL_IDENTIFIER': 'stay_id'}, inplace=True)\n",
        "# Random shuffle\n",
        "stays = model_data['stay_id'].unique()\n",
        "random.Random(5).shuffle(stays)\n",
        "model_data = model_data.set_index(\"stay_id\").loc[stays].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1711641411458
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# For only having one positive switch day per stay\n",
        "def lb_predicted_switch_day_fun(data):\n",
        "    # Convert to datetime\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "    # iv_treatment_length\n",
        "    cumcount = []\n",
        "    count = 0\n",
        "    pos = -1\n",
        "    flag = 0\n",
        "\n",
        "    for x in range(len(data)):\n",
        "        pos += 1\n",
        "        if pos == len(data) - 1:\n",
        "            cumcount.append(count) # add count to last one\n",
        "            break # end\n",
        "        elif pos == 0:\n",
        "            cumcount.append(count) # add 0 to first one\n",
        "            count += 1\n",
        "        elif data.iloc[x]['stay_id'] == data.iloc[x+1]['stay_id']:\n",
        "            if data.iloc[x]['lb_prediction'] == 0:\n",
        "                cumcount.append(count)\n",
        "                count += 1\n",
        "            elif flag == 1:\n",
        "                cumcount.append(999)\n",
        "                count = 0\n",
        "                flag = 1\n",
        "            elif data.iloc[x]['stay_id'] != data.iloc[x-1]['stay_id']:\n",
        "                if data.iloc[x]['lb_prediction'] == 1:\n",
        "                    cumcount.append(count)\n",
        "                    count += 1\n",
        "                else:\n",
        "                    cumcount.append(999)\n",
        "                    count = 0\n",
        "            else:\n",
        "                cumcount.append(count)\n",
        "                count = 0\n",
        "                flag = 1\n",
        "        else:\n",
        "            if data.iloc[x]['lb_prediction'] == 0:\n",
        "                cumcount.append(count)\n",
        "                count = 0\n",
        "                flag = 0\n",
        "            elif flag == 1:\n",
        "                cumcount.append(999)\n",
        "                count = 0\n",
        "                flag = 0\n",
        "            else:\n",
        "                cumcount.append(count)\n",
        "                count = 0\n",
        "                flag = 0\n",
        "\n",
        "    print(len(cumcount))\n",
        "\n",
        "    data['lb_predicted_switch_day'] = cumcount\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1711641799902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Filter for those who switch\n",
        "test_stay_id_list = (model_data.groupby(['stay_id'])['po_flag'].nunique() > 1).where(lambda x : x==True).dropna().reset_index()['stay_id'].unique().tolist()\n",
        "filtered_test_data = model_data[model_data['stay_id'].isin(test_stay_id_list)]\n",
        "\n",
        "\n",
        "# Find the day they actually switched\n",
        "test_switch_day = filtered_test_data[filtered_test_data['po_flag'] == 1].drop_duplicates(subset=['stay_id'], keep='first')\n",
        "test_switch_day = test_switch_day[['stay_id', 'iv_treatment_length']]\n",
        "test_switch_day.rename(columns={'iv_treatment_length': 'real_switch_day'}, inplace=True)\n",
        "test_switch_day.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Find day we predict they could switch\n",
        "filtered_test_data.reset_index(inplace=True, drop=True)\n",
        "filtered_test_data2 = filtered_test_data.drop(columns=['date', 'iv_treatment_length'])\n",
        "\n",
        "# Get predictions\n",
        "vitals_test_data = filtered_test_data2.iloc[:,2:255]\n",
        "demographics_test_data = filtered_test_data2.iloc[:,255:267]\n",
        "comorbidity_test_data = filtered_test_data2.iloc[:, 267:]\n",
        "comorbidity_test_data, comorbidity_test_mask = set_transformer_processing_fun(comorbidity_test_data, embedding)\n",
        "test_labels = filtered_test_data2[['po_flag']]\n",
        "\n",
        "test_dataset = MultiInputDataset([vitals_test_data, demographics_test_data], test_labels, comorbidity_test_data, comorbidity_test_mask)\n",
        "temp_test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)#, collate_fn=test_dataset.collate_fn_padd)\n",
        "\n",
        "\n",
        "test_loss, test_auroc, test_predictions, test_labels_out = evaluate(model, temp_test_dataloader, criterion)\n",
        "new_test_predictions = final_threshold_fun(test_predictions)\n",
        "\n",
        "filtered_test_data['lb_prediction'] = new_test_predictions\n",
        "\n",
        "# Find the day we predict they switched\n",
        "filtered_test_data = lb_predicted_switch_day_fun(filtered_test_data)\n",
        "\n",
        "test_lb_predicted_switch_day = filtered_test_data[filtered_test_data['lb_prediction'] == 1].drop_duplicates(subset=['stay_id'], keep='first')\n",
        "test_lb_predicted_switch_day = test_lb_predicted_switch_day[['stay_id', 'lb_predicted_switch_day']]\n",
        "test_lb_predicted_switch_day.reset_index(drop=True, inplace=True)\n",
        "test_lb_predicted_switch_day\n",
        "\n",
        "# Merge and work out difference\n",
        "test_switch_data = pd.merge(test_switch_day, test_lb_predicted_switch_day)\n",
        "test_switch_data['lb_difference'] = test_switch_data['lb_predicted_switch_day'] - test_switch_data['real_switch_day'] #- test_switch_data['lb_predicted_switch_day']\n",
        "\n",
        "lb_percentage_agree = len(test_switch_data[test_switch_data['lb_difference'] == 0])/len(test_switch_data)\n",
        "lb_percentage_early = len(test_switch_data[test_switch_data['lb_difference'] < 0])/len(test_switch_data)\n",
        "lb_percentage_late = len(test_switch_data[test_switch_data['lb_difference'] > 0])/len(test_switch_data)\n",
        "\n",
        "lb_percentage_agree\n",
        "lb_percentage_late\n",
        "lb_percentage_early"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Re-train and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1710865527126
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    batch_prediction_list = []\n",
        "    batch_label_list = []\n",
        "\n",
        "    for batch_idx, sample in enumerate(tqdm(dataloader)):\n",
        "        labels = sample[\"labels\"]\n",
        "        features = sample[\"features\"]\n",
        "        batch_mask = sample[\"mask\"]\n",
        "        features = [data.float() for data in features]\n",
        "        features = [data.to(device=device) for data in features]\n",
        "        labels = labels.float()\n",
        "        labels = labels.to(device=device)\n",
        "        batch_mask = batch_mask.to(device)\n",
        "\n",
        "        # zero the gradients calculated from the last batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run model\n",
        "        output = model(features, batch_mask)\n",
        "        \n",
        "        # Generate loss\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # update the parameters of our model by doing an optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Get predictions for performance calc - masking outputs and labels\n",
        "        sig = torch.nn.Sigmoid()\n",
        "        output = sig(output)      \n",
        "        np_predictions = output.cpu().detach().numpy()\n",
        "        np_labels = labels.cpu().detach().numpy()\n",
        "\n",
        "        np_predictions = np_predictions.squeeze()\n",
        "        np_labels = np_labels.squeeze()\n",
        "\n",
        "        np_predictions = np_predictions.flatten()\n",
        "        np_labels = np_labels.flatten()\n",
        "        \n",
        "        # Create list\n",
        "        for x in np_predictions:\n",
        "            batch_prediction_list.append(x)\n",
        "        for x in np_labels:\n",
        "            batch_label_list.append(x)\n",
        "\n",
        "    final_predictions = np.array(batch_prediction_list)\n",
        "\n",
        "    final_labels = np.array(batch_label_list)\n",
        "\n",
        "    try:\n",
        "        auroc = roc_auc_score(final_labels, final_predictions)\n",
        "    except:\n",
        "        auroc = np.nan\n",
        "    \n",
        "    try:\n",
        "        final_loss = epoch_loss / len(dataloader)\n",
        "    except:\n",
        "        final_loss = np.nan\n",
        "\n",
        "    return final_loss, auroc, final_predictions, final_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1710865531035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Chronic_switch_model(\n",
              "  (final_layers): Sequential(\n",
              "    (0): Linear(in_features=268, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.1, inplace=False)\n",
              "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (vital_model): Initial_vitals_model(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Linear(in_features=253, out_features=512, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.1, inplace=False)\n",
              "      (4): Linear(in_features=512, out_features=128, bias=True)\n",
              "      (5): ReLU()\n",
              "      (6): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (set_transformer): SetTransformer(\n",
              "    (enc): Sequential(\n",
              "      (0): ISAB(\n",
              "        (mab0): MAB0(\n",
              "          (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_k): Linear(in_features=128, out_features=160, bias=True)\n",
              "          (fc_v): Linear(in_features=128, out_features=160, bias=True)\n",
              "          (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "        )\n",
              "        (mab1): MAB(\n",
              "          (fc_q): Linear(in_features=128, out_features=160, bias=True)\n",
              "          (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_v): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (1): ISAB(\n",
              "        (mab0): MAB0(\n",
              "          (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_v): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "        )\n",
              "        (mab1): MAB(\n",
              "          (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_v): Linear(in_features=160, out_features=160, bias=True)\n",
              "          (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (isab): ISAB(\n",
              "      (mab0): MAB0(\n",
              "        (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_k): Linear(in_features=128, out_features=160, bias=True)\n",
              "        (fc_v): Linear(in_features=128, out_features=160, bias=True)\n",
              "        (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "      )\n",
              "      (mab1): MAB(\n",
              "        (fc_q): Linear(in_features=128, out_features=160, bias=True)\n",
              "        (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_v): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (pma): PMA(\n",
              "      (mab): MAB0(\n",
              "        (fc_q): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_k): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_v): Linear(in_features=160, out_features=160, bias=True)\n",
              "        (fc_o): Linear(in_features=160, out_features=160, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (dec): Sequential(\n",
              "      (0): Linear(in_features=160, out_features=128, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (demographics): Linear(in_features=12, out_features=12, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 1,126,583 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "final_input_dim = 268\n",
        "final_output_dim = 1\n",
        "final_hid_dim = 512\n",
        "final_hid_dim2 = 128\n",
        "demographics_input_dim = 12\n",
        "demographics_output_dim = 12\n",
        "vital_input_dim = 253\n",
        "vital_hid_dim = 512\n",
        "vital_output_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "# Define model\n",
        "model = Chronic_switch_model(\n",
        "    final_input_dim, \n",
        "    final_output_dim, \n",
        "    final_hid_dim, \n",
        "    final_hid_dim2,\n",
        "    demographics_input_dim,\n",
        "    demographics_output_dim,\n",
        "    vital_input_dim, \n",
        "    vital_hid_dim, \n",
        "    vital_output_dim, \n",
        "    dropout).to(device)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1710865536096
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class StratifiedKFold3(StratifiedKFold):\n",
        "\n",
        "    def split(self, X, y, groups=None):\n",
        "        s = super().split(X, y, groups)\n",
        "        for train_indxs, test_indxs in s:\n",
        "            y_train = y[train_indxs]\n",
        "            train_indxs, cv_indxs = train_test_split(train_indxs,stratify=y_train, test_size=(1 / (self.n_splits - 1)), random_state=0)\n",
        "            yield train_indxs, cv_indxs, test_indxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1710865538344
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to split data so even distribution between val and test\n",
        "def cv_data_fun(data, n_cv=10):\n",
        "    X = data.iloc[:, 2:]\n",
        "    y = data['po_flag']\n",
        "    g = StratifiedKFold3(n_cv).split(X,y)\n",
        "    return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1710865540096
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def new_threshold_fun(predictions, bound=0.5):\n",
        "    new_predictions = [1 if a_ >= bound else 0 for a_ in predictions]\n",
        "    return new_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1710865542250
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Function to train and eval model \n",
        "def cv_run_2023_fun(data, model):\n",
        "\n",
        "    overall_best_test_auroc = 0\n",
        "\n",
        "    actual_test_auroc_results = []\n",
        "\n",
        "    test_auroc_results = []\n",
        "    test_accuracy_results = []\n",
        "    test_balanced_accuracy_results = []\n",
        "    test_recall_results = []\n",
        "    test_precision_results = []\n",
        "    test_f1_results = []\n",
        "    test_auprc_results = []\n",
        "    test_cm_results = []\n",
        "    test_true_positive_rate_results = []\n",
        "    test_fasle_positive_rate_results = []\n",
        "\n",
        "    final_threshold = 0\n",
        "\n",
        "    # Define batch size \n",
        "    batch_size = 512\n",
        "\n",
        "    # Define optimizer and learning_rate\n",
        "    learning_rate = 0.0001\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Define epochs and clip\n",
        "    N_EPOCHS = 10\n",
        "    CLIP = 1\n",
        "\n",
        "    # Split into folds\n",
        "    split_generator = cv_data_fun(data)\n",
        "\n",
        "    # Iterate through folds\n",
        "    for x in range(N_EPOCHS): # Note this only works as number of pslits and epocs are the same\n",
        "        train_idx, val_idx, test_idx = next(split_generator)\n",
        "\n",
        "        # Get train val and test\n",
        "        train_data = data.loc[train_idx]\n",
        "        valid_data = data.loc[val_idx]\n",
        "        test_data = data.loc[test_idx]\n",
        "\n",
        "        #Apply smote - crashes with these features\n",
        "        #train_data = smote_fun(train_data)\n",
        "\n",
        "        # Split up dfs\n",
        "        vitals_train_data = train_data.iloc[:,2:255]\n",
        "        demographics_train_data = train_data.iloc[:,255:267]\n",
        "        comorbidity_train_data = train_data.iloc[:, 267:]\n",
        "\n",
        "        vitals_valid_data = valid_data.iloc[:,2:255]\n",
        "        demographics_valid_data = valid_data.iloc[:,255:267]\n",
        "        comorbidity_valid_data = valid_data.iloc[:, 267:]\n",
        "\n",
        "        vitals_test_data = test_data.iloc[:,2:255]\n",
        "        demographics_test_data = test_data.iloc[:,255:267]\n",
        "        comorbidity_test_data = test_data.iloc[:, 267:]\n",
        "\n",
        "        # Initializing the weights of our model each fold\n",
        "        model.apply(init_weights)\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = train_data[['po_flag']]\n",
        "        valid_labels = valid_data[['po_flag']]\n",
        "        test_labels = test_data[['po_flag']]\n",
        "\n",
        "        # Preprocess comorbidity data\n",
        "        print('Working on set_transformer_processing_fun...')\n",
        "        comorbidity_train_data, comorbidity_train_mask = set_transformer_processing_fun(comorbidity_train_data, embedding)\n",
        "        comorbidity_valid_data, comorbidity_valid_mask = set_transformer_processing_fun(comorbidity_valid_data, embedding)\n",
        "        comorbidity_test_data, comorbidity_test_mask = set_transformer_processing_fun(comorbidity_test_data, embedding)\n",
        "        print('Done!')\n",
        "\n",
        "        # Define dataloaders\n",
        "        train_dataset =  MultiInputDataset([vitals_train_data, demographics_train_data], train_labels, comorbidity_train_data, comorbidity_train_mask)\n",
        "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "\n",
        "        valid_dataset = MultiInputDataset([vitals_valid_data, demographics_valid_data], valid_labels, comorbidity_valid_data, comorbidity_valid_mask)\n",
        "        valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch_size)\n",
        "\n",
        "        test_dataset = MultiInputDataset([vitals_test_data, demographics_test_data], test_labels, comorbidity_test_data, comorbidity_test_mask)\n",
        "        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
        "\n",
        "        # Run\n",
        "        best_valid_loss = float('inf')\n",
        "        best_valid_auroc = 0\n",
        "\n",
        "        optimal_threshold = 0\n",
        "\n",
        "        for epoch in range(N_EPOCHS):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            train_loss, train_auroc, train_predictions, train_labels_out = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "\n",
        "            valid_loss, valid_auroc, valid_predictions, valid_labels_out = evaluate(model, valid_dataloader, criterion)\n",
        "\n",
        "            end_time = time.time()\n",
        "            \n",
        "            fpr, tpr, thresholds = roc_curve(valid_labels_out, valid_predictions)\n",
        "            optimal_idx = np.argmax(tpr - fpr)\n",
        "            current_threshold = thresholds[optimal_idx]\n",
        "\n",
        "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "            print('Train AUROC:', train_auroc)\n",
        "            print('Valid AUROC:', valid_auroc)\n",
        "            print(train_predictions)\n",
        "            print(train_labels_out)\n",
        "            print('Train loss:', train_loss)\n",
        "            print('Valid loss:', valid_loss)\n",
        "            print(current_threshold)\n",
        "\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss = valid_loss\n",
        "                print('BEST VALID LOSS')\n",
        "\n",
        "            if valid_auroc > best_valid_auroc:\n",
        "                best_valid_auroc = valid_auroc\n",
        "                print('UPDATED BEST INTERMEDIATE MODEL')\n",
        "                torch.save(model.state_dict(), f'chronic_switch_model_intermediate_2023.pt')\n",
        "                optimal_threshold = current_threshold\n",
        "\n",
        "        # -----------------------------\n",
        "        # Evaluate best model on test set\n",
        "        # -----------------------------\n",
        "\n",
        "        model.load_state_dict(torch.load(f'chronic_switch_model_intermediate_2023.pt'))\n",
        "\n",
        "        test_loss, test_auroc, test_predictions, test_labels_out = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "        print('Test AUROC result:', test_auroc)\n",
        "        \n",
        "        new_test_predictions = new_threshold_fun(test_predictions, optimal_threshold)\n",
        "\n",
        "        test_accuracy = accuracy_score(test_labels_out, new_test_predictions)\n",
        "        test_balanced_accuracy = balanced_accuracy_score(test_labels_out, new_test_predictions)\n",
        "        test_recall = recall_score(test_labels_out, new_test_predictions)\n",
        "        test_precision = precision_score(test_labels_out, new_test_predictions)\n",
        "        test_f1 = f1_score(test_labels_out, new_test_predictions)\n",
        "        test_auprc = average_precision_score(test_labels_out, test_predictions)\n",
        "        test_cm = confusion_matrix(test_labels_out, new_test_predictions)\n",
        "        tn, fp, fn, tp = test_cm.ravel()\n",
        "        test_true_positive_rate = (tp / (tp + fn))\n",
        "        test_false_positive_rate = (fp / (fp + tn))\n",
        "\n",
        "        if test_auroc > overall_best_test_auroc:\n",
        "            overall_best_test_auroc = test_auroc\n",
        "            print('UPDATED BEST OVERALL MODEL')\n",
        "            torch.save(model.state_dict(), f'chronic_switch_model_2023.pt') # Hastag out when dont want to change\n",
        "            final_threshold = optimal_threshold\n",
        "        \n",
        "        actual_test_auroc_results.append(test_auroc)\n",
        "        \n",
        "        test_auroc_results.append(test_auroc)\n",
        "        test_accuracy_results.append(test_accuracy)\n",
        "        test_balanced_accuracy_results.append(test_balanced_accuracy)\n",
        "        test_recall_results.append(test_recall)\n",
        "        test_precision_results.append(test_precision)\n",
        "        test_f1_results.append(test_f1)\n",
        "        test_auprc_results.append(test_auprc)\n",
        "        test_cm_results.append(test_cm)\n",
        "        test_true_positive_rate_results.append(test_true_positive_rate)\n",
        "        test_fasle_positive_rate_results.append(test_false_positive_rate)\n",
        "\n",
        "    test_results = [test_auroc_results, test_accuracy_results,\n",
        "        test_balanced_accuracy_results,\n",
        "        test_recall_results,\n",
        "        test_precision_results,\n",
        "        test_f1_results,\n",
        "        test_auprc_results,\n",
        "        test_cm_results,\n",
        "        test_true_positive_rate_results,\n",
        "        test_fasle_positive_rate_results\n",
        "        ]\n",
        "    \n",
        "    return test_results, actual_test_auroc_results, final_threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1710866037070
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:04<00:00,  1.25it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.98it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.21it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.20it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.03it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.17it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.16it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.14it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.80it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.82it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.05it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.86it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.15it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.14it/s]\n",
            "100%|| 5/5 [00:04<00:00,  1.21it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.00it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.59it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.04it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.08it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.09it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.15it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.83it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.05it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.96it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.14it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.86it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.15it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.15it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.06it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.15it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.18it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.87it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.24it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.23it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.09it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.97it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.03it/s]\n",
            "100%|| 5/5 [00:04<00:00,  1.23it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.04it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.01it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.08it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.86it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.97it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.04it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.09it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.01it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.99it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.03it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.06it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.95it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.01it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.16it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.06it/s]\n",
            "100%|| 5/5 [00:04<00:00,  1.19it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.09it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.13it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.05it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.06it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.02it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.20it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.18it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.09it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.16it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.32it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.28it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.00it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.16it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.02it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.16it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.12it/s]\n",
            "/tmp/ipykernel_25850/3153145714.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.30it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.20it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.18it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.31it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.18it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.33it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.14it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.03it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.27it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.08it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.26it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.05it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.25it/s]\n",
            "100%|| 1/1 [00:00<00:00,  2.74it/s]\n",
            "100%|| 5/5 [00:03<00:00,  1.29it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n",
            "100%|| 1/1 [00:00<00:00,  3.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.522790606839811\n",
            "Valid AUROC: 0.5340277777777778\n",
            "[0.5433474 0.5232676 0.6712101 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.7142103910446167\n",
            "Valid loss: 0.6916259527206421\n",
            "0.50500834\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.663797341158463\n",
            "Valid AUROC: 0.7182692307692309\n",
            "[0.5       0.5       0.5628729 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6493540406227112\n",
            "Valid loss: 0.650580108165741\n",
            "0.55437297\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7356231185178322\n",
            "Valid AUROC: 0.7347222222222223\n",
            "[0.5709091 0.6438756 0.7150964 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6173887610435486\n",
            "Valid loss: 0.636620819568634\n",
            "0.58532774\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.754335911386415\n",
            "Valid AUROC: 0.7512019230769231\n",
            "[0.5        0.61641073 0.7705373  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6036199331283569\n",
            "Valid loss: 0.6374799013137817\n",
            "0.6325347\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7564086193269688\n",
            "Valid AUROC: 0.7534188034188034\n",
            "[0.5921609 0.7738258 0.7100295 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6013130784034729\n",
            "Valid loss: 0.6260125041007996\n",
            "0.6104971\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7788549571812842\n",
            "Valid AUROC: 0.7623397435897437\n",
            "[0.6439403  0.66132253 0.74398816 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5816869616508484\n",
            "Valid loss: 0.6165792346000671\n",
            "0.5742936\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7744595811519199\n",
            "Valid AUROC: 0.7630608974358974\n",
            "[0.5        0.84035236 0.76162803 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.58262779712677\n",
            "Valid loss: 0.6297162175178528\n",
            "0.5047395\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7757911718582431\n",
            "Valid AUROC: 0.767147435897436\n",
            "[0.5       0.8511831 0.8423368 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5775104641914368\n",
            "Valid loss: 0.6359186172485352\n",
            "0.7745007\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7783462736742304\n",
            "Valid AUROC: 0.7561431623931625\n",
            "[0.50860626 0.8468252  0.86372566 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5746976137161255\n",
            "Valid loss: 0.6038796901702881\n",
            "0.6041342\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7851098030280649\n",
            "Valid AUROC: 0.766025641025641\n",
            "[0.5        0.7862006  0.71256536 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5690810680389404\n",
            "Valid loss: 0.6190439462661743\n",
            "0.71905357\n",
            "Test AUROC result: 0.7610042735042735\n",
            "UPDATED BEST OVERALL MODEL\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.6376934061430941\n",
            "Valid AUROC: 0.7899839743589743\n",
            "[0.5183435 0.5658491 0.5       ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6690209150314331\n",
            "Valid loss: 0.6341498494148254\n",
            "0.51657736\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7202424321331075\n",
            "Valid AUROC: 0.7879807692307692\n",
            "[0.5       0.6503327 0.5882992 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6309792399406433\n",
            "Valid loss: 0.6032536625862122\n",
            "0.5695095\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7510722414039164\n",
            "Valid AUROC: 0.7873931623931625\n",
            "[0.51417357 0.70888376 0.7258982  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6142711639404297\n",
            "Valid loss: 0.5996825695037842\n",
            "0.53926986\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7464636272514149\n",
            "Valid AUROC: 0.7866452991452991\n",
            "[0.53501517 0.6176717  0.7159411  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6085496664047241\n",
            "Valid loss: 0.5977813005447388\n",
            "0.65337497\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7587905434108585\n",
            "Valid AUROC: 0.780982905982906\n",
            "[0.53416675 0.7605652  0.77994096 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5990832567214965\n",
            "Valid loss: 0.5970354676246643\n",
            "0.50122726\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7582589086381052\n",
            "Valid AUROC: 0.791826923076923\n",
            "[0.5       0.7847266 0.5       ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5993680000305176\n",
            "Valid loss: 0.5903578996658325\n",
            "0.54722923\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7551333635455448\n",
            "Valid AUROC: 0.7868055555555554\n",
            "[0.6547301 0.7554755 0.6958476 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5935848712921142\n",
            "Valid loss: 0.5917090773582458\n",
            "0.5146928\n",
            "Train AUROC: 0.7672775041708708\n",
            "Valid AUROC: 0.7980235042735042\n",
            "[0.5       0.8195791 0.6403333 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5912618398666382\n",
            "Valid loss: 0.5929735898971558\n",
            "0.6437038\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7603086653129592\n",
            "Valid AUROC: 0.7955662393162393\n",
            "[0.5        0.92520684 0.8614656  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5877323150634766\n",
            "Valid loss: 0.5957069396972656\n",
            "0.6617136\n",
            "Train AUROC: 0.7743097719812617\n",
            "Valid AUROC: 0.795352564102564\n",
            "[0.5       0.8140529 0.7973525 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.578885817527771\n",
            "Valid loss: 0.5967276692390442\n",
            "0.6670955\n",
            "Test AUROC result: 0.7623397435897435\n",
            "UPDATED BEST OVERALL MODEL\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.4999419958921408\n",
            "Valid AUROC: 0.5\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.693478274345398\n",
            "Valid loss: 0.6931471228599548\n",
            "1.5\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.49995451476434055\n",
            "Valid AUROC: 0.5277777777777778\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6929461836814881\n",
            "Valid loss: 0.6928817629814148\n",
            "0.5015502\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.5724087395081419\n",
            "Valid AUROC: 0.7502938034188034\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6830444693565368\n",
            "Valid loss: 0.644080638885498\n",
            "0.5767786\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7258233453597964\n",
            "Valid AUROC: 0.7537660256410257\n",
            "[0.67847675 0.5        0.63285863 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6366459369659424\n",
            "Valid loss: 0.6276776790618896\n",
            "0.61218107\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7367410538052781\n",
            "Valid AUROC: 0.7614850427350428\n",
            "[0.5        0.6643953  0.68588555 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6204088687896728\n",
            "Valid loss: 0.6211861968040466\n",
            "0.64595115\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7311042229494297\n",
            "Valid AUROC: 0.7562767094017093\n",
            "[0.5799603 0.6862969 0.6354054 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6205818057060242\n",
            "Valid loss: 0.6097992062568665\n",
            "0.55374837\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7426912737618626\n",
            "Valid AUROC: 0.7637286324786325\n",
            "[0.5831085  0.59757847 0.5        ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6042224764823914\n",
            "Valid loss: 0.6168135404586792\n",
            "0.6698123\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7548746401867483\n",
            "Valid AUROC: 0.7574519230769231\n",
            "[0.5834339  0.7239362  0.64914393 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6009312510490418\n",
            "Valid loss: 0.6050344109535217\n",
            "0.7132161\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7541168311229178\n",
            "Valid AUROC: 0.7629273504273505\n",
            "[0.63584816 0.8155157  0.5247028  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5993589758872986\n",
            "Valid loss: 0.6029146313667297\n",
            "0.62621504\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7659087741436883\n",
            "Valid AUROC: 0.760630341880342\n",
            "[0.5093224  0.5        0.70005333 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5918979525566102\n",
            "Valid loss: 0.6035895347595215\n",
            "0.6213399\n",
            "Test AUROC result: 0.8057692307692308\n",
            "UPDATED BEST OVERALL MODEL\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.5565114409973034\n",
            "Valid AUROC: 0.6975160256410257\n",
            "[0.5      0.5      0.513793 ... 0.5      0.5      0.5     ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6850772500038147\n",
            "Valid loss: 0.6750193238258362\n",
            "0.5006755\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6937650174304432\n",
            "Valid AUROC: 0.6970085470085471\n",
            "[0.5       0.5064644 0.5316187 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6444683194160461\n",
            "Valid loss: 0.6517091393470764\n",
            "0.52190363\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7333229843989815\n",
            "Valid AUROC: 0.735897435897436\n",
            "[0.5       0.5521193 0.5988169 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6265111088752746\n",
            "Valid loss: 0.6273773908615112\n",
            "0.61054343\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7454909108814871\n",
            "Valid AUROC: 0.7413194444444445\n",
            "[0.57985044 0.73404086 0.79598546 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6114522576332092\n",
            "Valid loss: 0.6248305439949036\n",
            "0.6656663\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7581888029537862\n",
            "Valid AUROC: 0.7460470085470086\n",
            "[0.5775173  0.70114404 0.5        ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6033047199249267\n",
            "Valid loss: 0.6137494444847107\n",
            "0.60110545\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7541239251504978\n",
            "Valid AUROC: 0.7492521367521368\n",
            "[0.56363773 0.5        0.82475585 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5975353717803955\n",
            "Valid loss: 0.6146529316902161\n",
            "0.64530486\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7594039681486506\n",
            "Valid AUROC: 0.7454059829059828\n",
            "[0.51591325 0.72626996 0.88193595 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5980866312980652\n",
            "Valid loss: 0.6106529831886292\n",
            "0.6121065\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7785186168148484\n",
            "Valid AUROC: 0.7546474358974359\n",
            "[0.51970977 0.7280552  0.8795988  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5874164581298829\n",
            "Valid loss: 0.6100165843963623\n",
            "0.5262629\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7650474757363394\n",
            "Valid AUROC: 0.7613514957264956\n",
            "[0.5        0.7703916  0.67894197 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5918192625045776\n",
            "Valid loss: 0.6061554551124573\n",
            "0.55490863\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7744049154099805\n",
            "Valid AUROC: 0.7536858974358974\n",
            "[0.51412725 0.5        0.80024505 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5846953868865967\n",
            "Valid loss: 0.6105974912643433\n",
            "0.50334924\n",
            "Test AUROC result: 0.7135416666666667\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.560482427259093\n",
            "Valid AUROC: 0.7080128205128206\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6829115152359009\n",
            "Valid loss: 0.6607391238212585\n",
            "0.538886\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6941197188094387\n",
            "Valid AUROC: 0.7498931623931624\n",
            "[0.56795096 0.57597154 0.6227343  ... 0.5        0.5        0.6310824 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6541065812110901\n",
            "Valid loss: 0.6360571980476379\n",
            "0.5987882\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7378406280801642\n",
            "Valid AUROC: 0.7533119658119658\n",
            "[0.5        0.61549693 0.5670467  ... 0.5        0.5        0.8048021 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.629874074459076\n",
            "Valid loss: 0.6264575719833374\n",
            "0.51805574\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7302775600885001\n",
            "Valid AUROC: 0.7522435897435897\n",
            "[0.5       0.730341  0.6014605 ... 0.5       0.5       0.6006536]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6240269541740417\n",
            "Valid loss: 0.6151384711265564\n",
            "0.54596895\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7572991284361175\n",
            "Valid AUROC: 0.7532318376068375\n",
            "[0.60023147 0.6638034  0.69087064 ... 0.5        0.5        0.5564352 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.602965486049652\n",
            "Valid loss: 0.610315203666687\n",
            "0.55617714\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7636553771477168\n",
            "Valid AUROC: 0.7530448717948717\n",
            "[0.53678936 0.733388   0.583806   ... 0.5        0.5        0.78169113]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5995841264724732\n",
            "Valid loss: 0.6081911325454712\n",
            "0.5677526\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7731355017689167\n",
            "Valid AUROC: 0.761030982905983\n",
            "[0.54308736 0.75799346 0.5        ... 0.5        0.5        0.60916495]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.594225037097931\n",
            "Valid loss: 0.6096362471580505\n",
            "0.6436628\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7738657693139074\n",
            "Valid AUROC: 0.7585470085470085\n",
            "[0.5356743 0.8714342 0.6477647 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5887359619140625\n",
            "Valid loss: 0.6074082851409912\n",
            "0.6273903\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7807060810839006\n",
            "Valid AUROC: 0.7519497863247864\n",
            "[0.5        0.73392546 0.7455248  ... 0.5        0.5        0.6610934 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5858609080314636\n",
            "Valid loss: 0.6043388247489929\n",
            "0.5540287\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7864655968873075\n",
            "Valid AUROC: 0.7622863247863247\n",
            "[0.5       0.7981167 0.6715684 ... 0.5       0.5       0.5179925]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5783099889755249\n",
            "Valid loss: 0.6060979962348938\n",
            "0.5833979\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Test AUROC result: 0.7550747863247863\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.4965109903179043\n",
            "Valid AUROC: 0.573397435897436\n",
            "[0.5177692 0.5       0.5       ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.696473503112793\n",
            "Valid loss: 0.6904293298721313\n",
            "0.5041999\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6384624821919044\n",
            "Valid AUROC: 0.7356303418803418\n",
            "[0.5        0.5362691  0.5        ... 0.5        0.5813839  0.67794305]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6672921657562256\n",
            "Valid loss: 0.6486280560493469\n",
            "0.57690287\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7118898406013733\n",
            "Valid AUROC: 0.7477564102564102\n",
            "[0.5206478 0.7205823 0.5       ... 0.5       0.5       0.554307 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6402533769607544\n",
            "Valid loss: 0.6284147500991821\n",
            "0.59375364\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7368291032064171\n",
            "Valid AUROC: 0.7509882478632478\n",
            "[0.5        0.6186132  0.61909807 ... 0.5        0.5        0.6863449 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6229929208755494\n",
            "Valid loss: 0.6191821694374084\n",
            "0.58398265\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7563305850235899\n",
            "Valid AUROC: 0.7603632478632478\n",
            "[0.5        0.5        0.51160175 ... 0.5        0.5        0.5975562 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6112736225128174\n",
            "Valid loss: 0.6161941289901733\n",
            "0.63672364\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7434590979234529\n",
            "Valid AUROC: 0.7641559829059829\n",
            "[0.5       0.7187758 0.5787413 ... 0.5       0.5       0.5989744]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6103842616081238\n",
            "Valid loss: 0.6180140376091003\n",
            "0.65295756\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.754182763849837\n",
            "Valid AUROC: 0.7381410256410256\n",
            "[0.5        0.78640485 0.73209184 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6043257594108582\n",
            "Valid loss: 0.6164028644561768\n",
            "0.5183192\n",
            "Train AUROC: 0.7536077303201242\n",
            "Valid AUROC: 0.7565705128205128\n",
            "[0.5        0.5        0.6769074  ... 0.5        0.5        0.58787626]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6017056107521057\n",
            "Valid loss: 0.6082597970962524\n",
            "0.5744709\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7660765270311664\n",
            "Valid AUROC: 0.7601495726495727\n",
            "[0.5       0.7722236 0.6948107 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5892702579498291\n",
            "Valid loss: 0.6089030504226685\n",
            "0.6060046\n",
            "Train AUROC: 0.7540751015489183\n",
            "Valid AUROC: 0.7619123931623931\n",
            "[0.5       0.5       0.8587361 ... 0.5       0.5       0.5711966]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.593815803527832\n",
            "Valid loss: 0.6061413288116455\n",
            "0.58718634\n",
            "BEST VALID LOSS\n",
            "Test AUROC result: 0.7635149572649572\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.5137820264048053\n",
            "Valid AUROC: 0.6762553418803418\n",
            "[0.5838842 0.5407245 0.5988255 ... 0.5       0.5       0.6074386]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6945955634117127\n",
            "Valid loss: 0.678084671497345\n",
            "0.50345784\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6829078168672609\n",
            "Valid AUROC: 0.732852564102564\n",
            "[0.5147462  0.5        0.52815294 ... 0.5        0.62745035 0.7338231 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6579537391662598\n",
            "Valid loss: 0.6593446135520935\n",
            "0.5003157\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7145713830265793\n",
            "Valid AUROC: 0.7525106837606838\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6388873100280762\n",
            "Valid loss: 0.6283776760101318\n",
            "0.52216965\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.734852373286062\n",
            "Valid AUROC: 0.7601495726495726\n",
            "[0.62785965 0.5        0.7899521  ... 0.5        0.5        0.77882904]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6241635084152222\n",
            "Valid loss: 0.6212447285652161\n",
            "0.5901815\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7460997453661397\n",
            "Valid AUROC: 0.7615384615384615\n",
            "[0.5        0.5        0.78490037 ... 0.5        0.5        0.7573712 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6150585055351258\n",
            "Valid loss: 0.6164069771766663\n",
            "0.59076416\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7477514019050386\n",
            "Valid AUROC: 0.763888888888889\n",
            "[0.6786365  0.5        0.871161   ... 0.5        0.5        0.70514613]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6110106110572815\n",
            "Valid loss: 0.6130216121673584\n",
            "0.62421674\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7552706538440032\n",
            "Valid AUROC: 0.7593215811965812\n",
            "[0.574394   0.5        0.9405597  ... 0.5        0.5        0.75270313]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6068580985069275\n",
            "Valid loss: 0.6194539070129395\n",
            "0.5082778\n",
            "Train AUROC: 0.7568926823853627\n",
            "Valid AUROC: 0.7583867521367521\n",
            "[0.5        0.5        0.8881237  ... 0.5        0.5        0.71323705]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5999014496803283\n",
            "Valid loss: 0.6148315072059631\n",
            "0.5711557\n",
            "Train AUROC: 0.7461356327997789\n",
            "Valid AUROC: 0.765972222222222\n",
            "[0.5       0.5200642 0.5       ... 0.5       0.5       0.693423 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.597182548046112\n",
            "Valid loss: 0.6117554903030396\n",
            "0.69604224\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7613089232017266\n",
            "Valid AUROC: 0.7618589743589743\n",
            "[0.51956874 0.5        0.87819487 ... 0.5        0.5        0.56555605]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5917750239372254\n",
            "Valid loss: 0.6100112199783325\n",
            "0.5804929\n",
            "BEST VALID LOSS\n",
            "Test AUROC result: 0.7998397435897435\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.48248234213076213\n",
            "Valid AUROC: 0.5\n",
            "[0.5        0.51751775 0.5022166  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6962587118148804\n",
            "Valid loss: 0.6931471228599548\n",
            "1.5\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.5014730539621813\n",
            "Valid AUROC: 0.5\n",
            "[0.5      0.535779 0.5      ... 0.5      0.5      0.5     ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6930162310600281\n",
            "Valid loss: 0.6931471228599548\n",
            "1.5\n",
            "Train AUROC: 0.5006000712741124\n",
            "Valid AUROC: 0.5\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6930645346641541\n",
            "Valid loss: 0.6931471228599548\n",
            "1.5\n",
            "Train AUROC: 0.5032294517318191\n",
            "Valid AUROC: 0.5\n",
            "[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6931152224540711\n",
            "Valid loss: 0.6931471228599548\n",
            "1.5\n",
            "Train AUROC: 0.5149550447299305\n",
            "Valid AUROC: 0.6065705128205128\n",
            "[0.5        0.5        0.5        ... 0.5        0.50863343 0.6229042 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6931535840034485\n",
            "Valid loss: 0.6857422590255737\n",
            "0.5073006\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6254628852995892\n",
            "Valid AUROC: 0.6822649572649573\n",
            "[0.5121897  0.5        0.5        ... 0.5        0.5        0.75774926]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.669515335559845\n",
            "Valid loss: 0.6680333614349365\n",
            "0.50685155\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.6898937648505121\n",
            "Valid AUROC: 0.7197649572649574\n",
            "[0.5       0.5       0.6446541 ... 0.5       0.5       0.7850763]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6474270701408387\n",
            "Valid loss: 0.6506447196006775\n",
            "0.5167467\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7214208753028524\n",
            "Valid AUROC: 0.7673076923076922\n",
            "[0.5       0.5       0.6787494 ... 0.5       0.5       0.5      ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6231412172317505\n",
            "Valid loss: 0.6138656139373779\n",
            "0.5716132\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7423812230270466\n",
            "Valid AUROC: 0.7686965811965811\n",
            "[0.5        0.5        0.78449005 ... 0.5        0.5        0.6855115 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6133315205574036\n",
            "Valid loss: 0.6199847459793091\n",
            "0.5185628\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.767123104747073\n",
            "Valid AUROC: 0.7782852564102564\n",
            "[0.52445465 0.5031041  0.7846736  ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5980544447898865\n",
            "Valid loss: 0.6013693809509277\n",
            "0.55068886\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Test AUROC result: 0.7025106837606838\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.5905665290425318\n",
            "Valid AUROC: 0.7620192307692307\n",
            "[0.6006538  0.7836528  0.63996184 ... 0.71981406 0.5        0.6368145 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6828864097595215\n",
            "Valid loss: 0.6470310091972351\n",
            "0.5366132\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7043422125520891\n",
            "Valid AUROC: 0.7695245726495727\n",
            "[0.5        0.5        0.5        ... 0.6794723  0.5        0.61144215]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6475747346878051\n",
            "Valid loss: 0.6231022477149963\n",
            "0.5557941\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7298978209651049\n",
            "Valid AUROC: 0.7735844017094017\n",
            "[0.5        0.573828   0.74542785 ... 0.6897406  0.5        0.5992659 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6266731977462768\n",
            "Valid loss: 0.6132376194000244\n",
            "0.59522283\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7492778697219391\n",
            "Valid AUROC: 0.7757211538461539\n",
            "[0.5        0.6449997  0.75918984 ... 0.5756847  0.5        0.63798064]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6139482855796814\n",
            "Valid loss: 0.6018027067184448\n",
            "0.58030605\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7493767688123179\n",
            "Valid AUROC: 0.7728899572649574\n",
            "[0.5        0.62547606 0.83801925 ... 0.5376979  0.5        0.59023637]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6107033729553223\n",
            "Valid loss: 0.6031990647315979\n",
            "0.51848686\n",
            "Train AUROC: 0.7428152105966411\n",
            "Valid AUROC: 0.7846688034188034\n",
            "[0.5        0.5        0.78609496 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.608397102355957\n",
            "Valid loss: 0.5925089120864868\n",
            "0.57712674\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7641327634742708\n",
            "Valid AUROC: 0.7870192307692309\n",
            "[0.5        0.63967246 0.8710968  ... 0.5684508  0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5979460597038269\n",
            "Valid loss: 0.5913887619972229\n",
            "0.62251085\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7549772949387868\n",
            "Valid AUROC: 0.7616987179487179\n",
            "[0.5        0.7264637  0.90999264 ... 0.5940949  0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5975307941436767\n",
            "Valid loss: 0.601069450378418\n",
            "0.55629414\n",
            "Train AUROC: 0.7582601605253253\n",
            "Valid AUROC: 0.7852564102564102\n",
            "[0.5        0.65954095 0.8073744  ... 0.5431067  0.5        0.63724774]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5967854619026184\n",
            "Valid loss: 0.5938964486122131\n",
            "0.504564\n",
            "Train AUROC: 0.7617750425433006\n",
            "Valid AUROC: 0.7825587606837606\n",
            "[0.5        0.54664254 0.77199966 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5919805288314819\n",
            "Valid loss: 0.5894218683242798\n",
            "0.5198756\n",
            "BEST VALID LOSS\n",
            "Test AUROC result: 0.7637286324786325\n",
            "Working on set_transformer_processing_fun...\n",
            "Done!\n",
            "Train AUROC: 0.5898984208066239\n",
            "Valid AUROC: 0.7520299145299145\n",
            "[0.8529665  0.76214224 0.71238434 ... 0.5927752  0.68004006 0.70368445]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6938161611557007\n",
            "Valid loss: 0.6534900665283203\n",
            "0.53510857\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7040331196581197\n",
            "Valid AUROC: 0.7811965811965812\n",
            "[0.5        0.5459504  0.5672341  ... 0.62446487 0.6546307  0.64160115]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6512072086334229\n",
            "Valid loss: 0.6400629878044128\n",
            "0.50077814\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7356203258547008\n",
            "Valid AUROC: 0.7836538461538463\n",
            "[0.5        0.6089997  0.5723356  ... 0.58917826 0.70321274 0.6671503 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6375122427940368\n",
            "Valid loss: 0.6288800239562988\n",
            "0.50321233\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7440851195245726\n",
            "Valid AUROC: 0.7915064102564102\n",
            "[0.5       0.5023403 0.6537494 ... 0.580427  0.5       0.6448775]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6207822322845459\n",
            "Valid loss: 0.6062321066856384\n",
            "0.5415142\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.748352781116453\n",
            "Valid AUROC: 0.7952457264957266\n",
            "[0.5        0.64718145 0.6879094  ... 0.5336627  0.62462133 0.5385123 ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6131122827529907\n",
            "Valid loss: 0.6053453683853149\n",
            "0.5200381\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7519898504273503\n",
            "Valid AUROC: 0.7892628205128205\n",
            "[0.5        0.64263445 0.75027835 ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.606416916847229\n",
            "Valid loss: 0.6069550514221191\n",
            "0.50229436\n",
            "Train AUROC: 0.7494950253739316\n",
            "Valid AUROC: 0.790892094017094\n",
            "[0.5        0.5        0.7342268  ... 0.54901844 0.7606731  0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.6054275393486023\n",
            "Valid loss: 0.5985605120658875\n",
            "0.50710696\n",
            "BEST VALID LOSS\n",
            "Train AUROC: 0.7591554821047009\n",
            "Valid AUROC: 0.7685363247863248\n",
            "[0.5        0.61182076 0.5        ... 0.51384103 0.66402835 0.61247224]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5951785445213318\n",
            "Valid loss: 0.6089136004447937\n",
            "0.5053631\n",
            "Train AUROC: 0.7569169003739316\n",
            "Valid AUROC: 0.8001068376068377\n",
            "[0.5        0.5        0.7300516  ... 0.5        0.63967305 0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5935069799423218\n",
            "Valid loss: 0.5871097445487976\n",
            "0.53642595\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Train AUROC: 0.7696910890758547\n",
            "Valid AUROC: 0.8091613247863247\n",
            "[0.5        0.60072154 0.5        ... 0.5        0.5        0.5       ]\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "Train loss: 0.5824461102485656\n",
            "Valid loss: 0.5770549774169922\n",
            "0.52660125\n",
            "BEST VALID LOSS\n",
            "UPDATED BEST INTERMEDIATE MODEL\n",
            "Test AUROC result: 0.7351198419900711\n"
          ]
        }
      ],
      "source": [
        "# Run model\n",
        "test_results, actual_test_auroc_results, final_threshold = cv_run_2023_fun(model_data, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1710866037303
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Save \n",
        "with open(\"chronic_switch_test_results_2023_2\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(test_results, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1710866037542
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean test_auroc: 0.7562443559938788\n",
            "std test_auroc: 0.031113337711807052\n",
            "test_auroc 2.5th percentile: 0.70499265491453\n",
            "test_auroc 97.5th percentile: 0.8044350961538461\n",
            "mean test_accuracy: 0.7036496350364964\n",
            "std test_accuracy: 0.01895000727846303\n",
            "test_accuracy 2.5th percentile: 0.6699817518248175\n",
            "test_accuracy 97.5th percentile: 0.7359489051094891\n",
            "mean test_balanced_accuracy: 0.7070044025101276\n",
            "std test_balanced_accuracy: 0.019052215906055027\n",
            "test_balanced_accuracy 2.5th percentile: 0.6734041132478633\n",
            "test_balanced_accuracy 97.5th percentile: 0.7389129273504274\n",
            "mean test_recall: 0.6407439782439782\n",
            "std test_recall: 0.05060912301847776\n",
            "test_recall 2.5th percentile: 0.56875\n",
            "test_recall 97.5th percentile: 0.7121527777777777\n",
            "mean test_precision: 0.7610007716887507\n",
            "std test_precision: 0.03852150103709388\n",
            "test_precision 2.5th percentile: 0.7123214285714285\n",
            "test_precision 97.5th percentile: 0.8368764302059497\n",
            "mean test_f1: 0.6934708223793875\n",
            "std test_f1: 0.024892712631346316\n",
            "test_f1 2.5th percentile: 0.658955223880597\n",
            "test_f1 97.5th percentile: 0.7369544973274994\n",
            "mean test_auprc: 0.7594509134855476\n",
            "std test_auprc: 0.04246998473008413\n",
            "test_auprc 2.5th percentile: 0.696568813362417\n",
            "test_auprc 97.5th percentile: 0.825496132059587\n",
            "mean test_tpr: 0.6407439782439782\n",
            "std test_tpr: 0.05060912301847776\n",
            "test_tpr 2.5th percentile: 0.56875\n",
            "test_tpr 97.5th percentile: 0.7121527777777777\n",
            "mean test_fpr: 0.22673517322372283\n",
            "std test_fpr: 0.054573783132874545\n",
            "test_fpr 2.5th percentile: 0.12673076923076923\n",
            "test_fpr 97.5th percentile: 0.2982692307692308\n"
          ]
        }
      ],
      "source": [
        "analyze_results_fun(test_results)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
